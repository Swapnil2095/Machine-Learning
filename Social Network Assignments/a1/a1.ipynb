{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#You should not use any imports not listed here:\n",
    "from collections import Counter, defaultdict, deque\n",
    "import copy\n",
    "import math\n",
    "import networkx as nx\n",
    "import urllib.request\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Community Detection\n",
    "\n",
    "def example_graph():\n",
    "    \"\"\"\n",
    "    Create the example graph from class. Used for testing.\n",
    "    Do not modify.\n",
    "    \"\"\"\n",
    "    g = nx.Graph()\n",
    "    g.add_edges_from([('A', 'B'), ('A', 'C'), ('B', 'C'), ('B', 'D'), ('D', 'E'), ('D', 'F'), ('D', 'G'), ('E', 'F'), ('G', 'F')])\n",
    "    return g\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bfs(graph, root, max_depth):\n",
    "    \"\"\"\n",
    "    Perform breadth-first search to compute the shortest paths from a root node to all\n",
    "    other nodes in the graph. To reduce running time, the max_depth parameter ends\n",
    "    the search after the specified depth.\n",
    "    E.g., if max_depth=2, only paths of length 2 or less will be considered.\n",
    "    This means that nodes greather than max_depth distance from the root will not\n",
    "    appear in the result.\n",
    "\n",
    "    You may use these two classes to help with this implementation:\n",
    "      https://docs.python.org/3.5/library/collections.html#collections.defaultdict\n",
    "      https://docs.python.org/3.5/library/collections.html#collections.deque\n",
    "\n",
    "    Params:\n",
    "      graph.......A networkx Graph\n",
    "      root........The root node in the search graph (a string). We are computing\n",
    "                  shortest paths from this node to all others.\n",
    "      max_depth...An integer representing the maximum depth to search.\n",
    "\n",
    "    Returns:\n",
    "      node2distances...dict from each node to the length of the shortest path from\n",
    "                       the root node\n",
    "      node2num_paths...dict from each node to the number of shortest paths from the\n",
    "                       root node that pass through this node.\n",
    "      node2parents.....dict from each node to the list of its parents in the search\n",
    "                       tree\n",
    "\n",
    "    In the doctests below, we first try with max_depth=5, then max_depth=2.\n",
    "\n",
    "    >>> node2distances, node2num_paths, node2parents = bfs(example_graph(), 'E', 5)\n",
    "    >>> sorted(node2distances.items())\n",
    "    [('A', 3), ('B', 2), ('C', 3), ('D', 1), ('E', 0), ('F', 1), ('G', 2)]\n",
    "    >>> sorted(node2num_paths.items())\n",
    "    [('A', 1), ('B', 1), ('C', 1), ('D', 1), ('E', 1), ('F', 1), ('G', 2)]\n",
    "    >>> sorted((node, sorted(parents)) for node, parents in node2parents.items())\n",
    "    [('A', ['B']), ('B', ['D']), ('C', ['B']), ('D', ['E']), ('F', ['E']), ('G', ['D', 'F'])]\n",
    "\n",
    "  \n",
    "=======\n",
    "\n",
    "    >>> node2distances, node2num_paths, node2parents = bfs(example_graph(), 'E', 2)\n",
    "    >>> sorted(node2distances.items())\n",
    "    [('B', 2), ('D', 1), ('E', 0), ('F', 1), ('G', 2)]\n",
    "    >>> sorted(node2num_paths.items())\n",
    "    [('B', 1), ('D', 1), ('E', 1), ('F', 1), ('G', 2)]\n",
    "    >>> sorted((node, sorted(parents)) for node, parents in node2parents.items())\n",
    "    [('B', ['D']), ('D', ['E']), ('F', ['E']), ('G', ['D', 'F'])]\n",
    "\n",
    "  \n",
    "    >>> node2distances, node2num_paths, node2parents = bfs(example_graph(), 'E', 1)\n",
    "    >>> sorted(node2distances.items())\n",
    "    [('D', 1), ('E', 0), ('F', 1)]\n",
    "    >>> sorted(node2num_paths.items())\n",
    "    [('D', 1), ('E', 1), ('F', 1)]\n",
    "    >>> sorted((node, sorted(parents)) for node, parents in node2parents.items())\n",
    "    [('D', ['E']), ('F', ['E'])]\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    ###TODO\n",
    "    \n",
    "    #print('%d-------->In bfs'%(max_depth))\n",
    "    \n",
    "    #print(graph.edges())\n",
    "    node2distances = dict() \n",
    "    node2num_paths = dict()\n",
    "    node2parents = dict() \n",
    "    depth_dict = dict() # keeping all depth and nodes present at that depth \n",
    "                        #-> helping to increment the depth variable\n",
    "\n",
    "    d = deque()\n",
    "    \n",
    "    depth_dict.setdefault(0,[]).append(root) # 0th depth -->root \n",
    " \n",
    "    node2distances[root]=0\n",
    "    node2num_paths[root]=1\n",
    "    depth = 0 # started with zero\n",
    "    \n",
    "    if(max_depth == 1) :\n",
    "    \n",
    "       nbr_list = graph.neighbors(root)\n",
    "       \n",
    "       for nbr in nbr_list :                             \n",
    "           node2distances[nbr] = 1 # node2distances[root]  + 1             \n",
    "           node2parents.setdefault(nbr,[]).append(root) \n",
    "           node2num_paths[nbr] = 1 # node2num_paths[root]\n",
    "\n",
    "    else:  \n",
    "      d.append(root) # 1st element in dequeue is root\n",
    "      while (len(d) >= 1 and depth < max_depth) :\n",
    " \n",
    "          current = d.popleft()                                      \n",
    "          nbr_list = graph.neighbors(current)\n",
    "          \n",
    "          #print('--->1Current=%s ---> list_lst=%s'%(current,nbr_list))                               \n",
    "          for nbr in nbr_list : \n",
    "              #print('1.Inside list nbr',nbr)\n",
    "              \n",
    "              # filling values to node2distances\n",
    "              if nbr not in node2distances.keys():\n",
    "                 #print('2.Inside list nbr',nbr)                 \n",
    "                 node2distances[nbr] = node2distances[current] + 1  \n",
    "                 d.append(nbr)  # adding to dequeue\n",
    "                 depth_dict.setdefault(depth+1,[]).append(nbr)  # additional dictionary\n",
    "              \n",
    "              # filling values to node2parents     \n",
    "              if nbr not in node2parents.keys() :\n",
    "                 node2parents.setdefault(nbr,[]).append(current) \n",
    "                    \n",
    "              else :\n",
    "                    if(node2distances[nbr] > node2distances[current]) :\n",
    "                       node2parents[nbr].append(current)\n",
    "             \n",
    "              # filling values to node2num_paths\n",
    "              if nbr not in node2num_paths.keys() :\n",
    "                  node2num_paths[nbr] = node2num_paths[current]\n",
    "                  \n",
    "              else :\n",
    "                  if(node2distances[nbr] > node2distances[current]) :    \n",
    "                     node2num_paths[nbr] += node2num_paths[current]\n",
    "                     \n",
    "                     \n",
    "          #print('depth_dict',depth_dict)\n",
    "          #print('node2distances',node2distances)\n",
    "          #print('node2parents',node2parents) \n",
    "          #print('node2num_paths',node2num_paths)  \n",
    "   \n",
    "          if depth in depth_dict.keys():      \n",
    "                 if(depth_dict[depth][-1] == current) : # if all nodes are traversed for particular depth then increment depth by 1                                              \n",
    "                      depth = depth + 1\n",
    "                      #print('2.depth',depth)\n",
    "                            \n",
    "            \n",
    "    if root in node2parents: \n",
    "        del node2parents[root]   \n",
    "     \n",
    "    node2distances[root]=0  # Extra care --not needed \n",
    "    node2num_paths[root]=1\n",
    " \n",
    "    #print('%d-------->Out bfs'%(max_depth))                   \n",
    "    #print('node2parents',node2parents) \n",
    "    #print('node2distances',node2distances) \n",
    "    #print('node2num_paths',node2num_paths) \n",
    "                   \n",
    "    return(node2distances,node2num_paths,node2parents)\n",
    "    \n",
    "    pass \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complexity_of_bfs(V, E, K):\n",
    "    \"\"\"\n",
    "    If V is the number of vertices in a graph, E is the number of\n",
    "    edges, and K is the max_depth of our approximate breadth-first\n",
    "    search algorithm, then what is the *worst-case* run-time of\n",
    "    this algorithm? As usual in complexity analysis, you can ignore\n",
    "    any constant factors. E.g., if you think the answer is 2V * E + 3log(K),\n",
    "    you would return V * E + math.log(K)\n",
    "\n",
    "\n",
    "    >>> v = complexity_of_bfs(13, 23, 7)\n",
    "    >>> type(v) == int or type(v) == float\n",
    "    True\n",
    "    \n",
    "\n",
    "     \n",
    "    \"\"\"\n",
    "    ###TODO\n",
    "    \n",
    "    #value1 = V * E + math.log(K)\n",
    "    #print('1.value=.2f',value1)\n",
    "   \n",
    "    #value2 = (V + E) * math.log(K)\n",
    "    #print('2.value=.2f',value2)\n",
    "    \n",
    "    return((V + E) * math.log(K))  # What I get after my complexity calcultion of my bfs algo\n",
    "    \n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bottom_up(root, node2distances, node2num_paths, node2parents):\n",
    "    \"\"\"\n",
    "    Compute the final step of the Girvan-Newman algorithm.\n",
    "    See p 352 From your text:\n",
    "    https://github.com/iit-cs579/main/blob/master/read/lru-10.pdf\n",
    "        The third and final step is to calculate for each edge e the sum\n",
    "        over all nodes Y of the fraction of shortest paths from the root\n",
    "        X to Y that go through e. This calculation involves computing this\n",
    "        sum for both nodes and edges, from the bottom. Each node other\n",
    "        than the root is given a credit of 1, representing the shortest\n",
    "        path to that node. This credit may be divided among nodes and\n",
    "        edges above, since there could be several different shortest paths\n",
    "        to the node. The rules for the calculation are as follows: ...\n",
    "\n",
    "    Params:\n",
    "      root.............The root node in the search graph (a string). We are computing\n",
    "                       shortest paths from this node to all others.\n",
    "      node2distances...dict from each node to the length of the shortest path from\n",
    "                       the root node\n",
    "      node2num_paths...dict from each node to the number of shortest paths from the\n",
    "                       root node that pass through this node.\n",
    "      node2parents.....dict from each node to the list of its parents in the search\n",
    "                       tree\n",
    "    Returns:\n",
    "      A dict mapping edges to credit value. Each key is a tuple of two strings\n",
    "      representing an edge (e.g., ('A', 'B')). Make sure each of these tuples\n",
    "      are sorted alphabetically (so, it's ('A', 'B'), not ('B', 'A')).\n",
    "\n",
    "      Any edges excluded from the results in bfs should also be exluded here.\n",
    "\n",
    "\n",
    "\n",
    "=======\n",
    "\n",
    "    >>> node2distances, node2num_paths, node2parents = bfs(example_graph(), 'E', 5)\n",
    "    >>> result = bottom_up('E', node2distances, node2num_paths, node2parents)\n",
    "    >>> sorted(result.items())\n",
    "    [(('A', 'B'), 1.0), (('B', 'C'), 1.0), (('B', 'D'), 3.0), (('D', 'E'), 4.5), (('D', 'G'), 0.5), (('E', 'F'), 1.5), (('F', 'G'), 0.5)]\n",
    "\n",
    "     \n",
    "    >>> node2distances, node2num_paths, node2parents = bfs(example_graph(), 'E', 2)\n",
    "    >>> result = bottom_up('E', node2distances, node2num_paths, node2parents)\n",
    "    >>> sorted(result.items())\n",
    "    [(('B', 'D'), 1.0), (('D', 'E'), 2.5), (('D', 'G'), 0.5), (('E', 'F'), 1.5), (('F', 'G'), 0.5)]\n",
    "  \n",
    "    >>> node2distances, node2num_paths, node2parents = bfs(example_graph(), 'E', 1)\n",
    "    >>> result = bottom_up('E', node2distances, node2num_paths, node2parents)\n",
    "    >>> sorted(result.items())\n",
    "    [(('D', 'E'), 1.0), (('E', 'F'), 1.0)]\n",
    "  \n",
    "\n",
    " \n",
    "    \"\"\"\n",
    "    ###TODO\n",
    "    #print('---------->In bootom_up')\n",
    "    #print('node2parents',node2parents) \n",
    "    #print('node2distances',node2distances) \n",
    "    #print('node2num_paths',node2num_paths)\n",
    "    node2distances1 = {} # keeping sorted(node2distances) i.e. node2distances1 = node2distances              \n",
    "    node2distances1 = sorted(node2distances.items(), key=lambda x:(-x[1],x[0]) , reverse=False)\n",
    "    #print('type of node2distances',type(node2distances1)) \n",
    "    #print('node2distances1',node2distances1)        \n",
    "\n",
    "    credit = {}       # node credit\n",
    "    credit_dict = {}  # path credit\n",
    "    children_list ={} # all children of particular node\n",
    "    \n",
    "\n",
    "    for node,distance in node2distances1 :\n",
    "            #print('------>node=%s distance=%d'%(node,distance))   \n",
    "            # finding children list for each node     \n",
    "            for node1 in node2parents.keys(): \n",
    "                for parent in node2parents[node1] :\n",
    "                    if (node == parent) :\n",
    "                        children_list.setdefault(node,[]).append(node1)            \n",
    "                 \n",
    "            #print('node=%s--->children_list=%s'%(node,children_list))     \n",
    "            if node not in children_list.keys(): # leaf node\n",
    "               credit[node] = 1\n",
    "               #print('----->node=%s ---> No child ---> credit = %s'%(node,credit))\n",
    "          \n",
    "            else: # not leaf node\n",
    "                 #print('2.node=%s ---> Has children=%s'%(node,children_list[node])) \n",
    "                 #print('cedit list for children - ',credit) \n",
    "                 children_credits = 0                              \n",
    "                                    \n",
    "                 for child in children_list[node]  : \n",
    "                      #print('2.credit of child %s = %d'%(child,credit[child]))                            \n",
    "                      path_credit = (credit[child])/(node2num_paths[child])\n",
    "                      #print('Path credit=%.2f'%(path_credit))                                                                                                    \n",
    "                      t = [node,child]  \n",
    "                      t.sort()    #keeping sorted order                                               \n",
    "                      credit_dict[t[0],t[1]] = path_credit\n",
    "                      \n",
    "                      children_credits += path_credit    # adding all child's credits\n",
    " \n",
    "                 #print('children_credits',children_credits)               \n",
    "                 credit[node] = children_credits + 1  # additional 1 for itself\n",
    "       \n",
    " \n",
    "\n",
    "    \n",
    "    #print('2.credit_dict',credit_dict) \n",
    "    #print('2.credit',credit)          \n",
    "    #print('---------->Out bootom_up')      \n",
    "    return(credit_dict)                \n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def approximate_betweenness(graph, max_depth):\n",
    "    \"\"\"\n",
    "    Compute the approximate betweenness of each edge, using max_depth to reduce\n",
    "    computation time in breadth-first search.\n",
    "\n",
    "    You should call the bfs and bottom_up functions defined above for each node\n",
    "    in the graph, and sum together the results. Be sure to divide by 2 at the\n",
    "    end to get the final betweenness.\n",
    "\n",
    "    Params:\n",
    "      graph.......A networkx Graph\n",
    "      max_depth...An integer representing the maximum depth to search.\n",
    "\n",
    "    Returns:\n",
    "      A dict mapping edges to betweenness. Each key is a tuple of two strings\n",
    "      representing an edge (e.g., ('A', 'B')). Make sure each of these tuples\n",
    "      are sorted alphabetically (so, it's ('A', 'B'), not ('B', 'A')).\n",
    "\n",
    "    >>> sorted(approximate_betweenness(example_graph(), 2).items())\n",
    "    [(('A', 'B'), 2.0), (('A', 'C'), 1.0), (('B', 'C'), 2.0), (('B', 'D'), 6.0), (('D', 'E'), 2.5), (('D', 'F'), 2.0), (('D', 'G'), 2.5), (('E', 'F'), 1.5), (('F', 'G'), 1.5)]\n",
    "\n",
    "\n",
    "\n",
    "    >>> sorted(approximate_betweenness(example_graph(), 1).items())\n",
    "    [(('A', 'B'), 1.0), (('A', 'C'), 1.0), (('B', 'C'), 1.0), (('B', 'D'), 1.0), (('D', 'E'), 1.0), (('D', 'F'), 1.0), (('D', 'G'), 1.0), (('E', 'F'), 1.0), (('F', 'G'), 1.0)]\n",
    "     \n",
    "     \"\"\"\n",
    "    ###TODO\n",
    "    #print('%d-----------> In approximate_betweenness'%(max_depth))   \n",
    "  \n",
    "    credit ={}      # final credit after summation from each node\n",
    "    credit_dict ={} # credit for each node\n",
    "\n",
    "    \n",
    "    for node in graph.nodes() :\n",
    "       \n",
    "       #print('For Node--->',node)\n",
    "       #print('--------------->credit_dict=',len(credit_dict))\n",
    "       node2distances, node2num_paths, node2parents = bfs(graph, node, max_depth)\n",
    "\n",
    "       credit_dict = bottom_up(node, node2distances, node2num_paths, node2parents)\n",
    "      \n",
    "       #print('--------------->credit_dict=',len(credit_dict))\n",
    "       #initailly credit is empty\n",
    "\n",
    "       for edge in credit_dict.keys() :\n",
    "           #print('edge=',edge)                                                                   \n",
    "           if edge not in credit.keys() :\n",
    "                credit[edge] = credit_dict[edge] #just store as it is if newly find  \n",
    "           else :                \n",
    "                credit[edge] =  credit[edge] + credit_dict[edge] #add if credit s caleculated in previous nodes  \n",
    "       \n",
    "       credit_dict.clear()   \n",
    "           \n",
    "    for edge in credit.keys() :\n",
    "         credit[edge] = credit[edge]/2  #****** final IMP step \n",
    "    \n",
    "    #print('--------------->credit',credit)\n",
    "    #print('%d-----------> Out approximate_betweenness'%(max_depth))        \n",
    "    return(credit)\n",
    "    \n",
    "    pass\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_approximation_always_right():\n",
    "    \"\"\"\n",
    "    Look at the doctests for approximate betweenness. In this example, the\n",
    "    edge with the highest betweenness was ('B', 'D') for both cases (when\n",
    "    max_depth=5 and max_depth=2).\n",
    "\n",
    "    Consider an arbitrary graph G. For all max_depth > 1, will it always be\n",
    "    the case that the edge with the highest betweenness will be the same\n",
    "    using either approximate_betweenness verses the exact computation?\n",
    "    Answer this question below.\n",
    "\n",
    "    In this function, you just need to return either the string 'yes' or 'no'\n",
    "    (no need to do any actual computations here).\n",
    "    >>> s = is_approximation_always_right()\n",
    "    >>> type(s)\n",
    "    <class 'str'>\n",
    "    \"\"\"\n",
    "    ###TODO\n",
    "\n",
    "    \n",
    "    answer = 'no'   # if max_depth parameter set then it gives different approximate_betweenness for some cases\n",
    "                    # like 2 nodes lot greater than the max_depth apart from each other \n",
    "    return (answer)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition_girvan_newman(graph, max_depth):\n",
    "    \"\"\"\n",
    "    Use your approximate_betweenness implementation to partition a graph.\n",
    "    Unlike in class, here you will not implement this recursively. Instead,\n",
    "    just remove edges until more than one component is created, then return\n",
    "    those components.\n",
    "    That is, compute the approximate betweenness of all edges, and remove\n",
    "    them until multiple comonents are created.\n",
    "\n",
    "    You only need to compute the betweenness once.\n",
    "    If there are ties in edge betweenness, break by edge name (e.g.,\n",
    "    (('A', 'B'), 1.0) comes before (('B', 'C'), 1.0)).\n",
    "\n",
    "    Note: the original graph variable should not be modified. Instead,\n",
    "    make a copy of the original graph prior to removing edges.\n",
    "    See the Graph.copy method https://networkx.github.io/documentation/development/reference/generated/networkx.Graph.copy.html\n",
    "    Params:\n",
    "      graph.......A networkx Graph\n",
    "      max_depth...An integer representing the maximum depth to search.\n",
    "\n",
    "    Returns:\n",
    "      A list of networkx Graph objects, one per partition.\n",
    "\n",
    "    >>> components = partition_girvan_newman(example_graph(), 5)\n",
    "    >>> components = sorted(components, key=lambda x: sorted(x.nodes())[0])\n",
    "    >>> sorted(components[0].nodes())\n",
    "    ['A', 'B', 'C']\n",
    "    >>> sorted(components[1].nodes())\n",
    "    ['D', 'E', 'F', 'G']\n",
    "<<<<<<< HEAD\n",
    "    \n",
    "    >>> components = partition_girvan_newman(example_graph(), 3)\n",
    "    >>> components = sorted(components, key=lambda x: sorted(x.nodes())[0])\n",
    "    >>> sorted(components[0].nodes())\n",
    "    ['A', 'B', 'C']\n",
    "    >>> sorted(components[1].nodes())\n",
    "    ['D', 'E', 'F', 'G']\n",
    "    \n",
    "    >>> components = partition_girvan_newman(example_graph(), 1)\n",
    "    >>> components = sorted(components, key=lambda x: sorted(x.nodes())[0])\n",
    "    >>> sorted(components[0].nodes())\n",
    "    ['A']\n",
    "    >>> sorted(components[1].nodes())\n",
    "    ['B', 'C', 'D', 'E', 'F', 'G']\n",
    "\n",
    "     \n",
    "      \n",
    "    \"\"\"\n",
    "    ###TODO\n",
    "    \n",
    "    #print('%d----------->In partition_girvan_newman'%(max_depth))\n",
    "\n",
    "    G=graph.copy()\n",
    "    #print('1.Number of Edges-',len(G.edges()))\n",
    "    \n",
    "    if (G.order() == 1) :\n",
    "        return [G]\n",
    "\n",
    "     # Each component is a separate community. We cluster each of these.\n",
    "    components = [c for c in nx.connected_component_subgraphs(G)]\n",
    "     \n",
    "     # sorted credits\n",
    "    eb = sorted((approximate_betweenness(G,max_depth)).items(), key=lambda x: (-x[1],x[0][0],x[0][1]))\n",
    "     \n",
    "    i = 0  # for next high credit if graph not divided into components not more than 1\n",
    "    while len(components)==1 and (i < len(eb)): #just want 2 components \n",
    "    \n",
    "        #print('1.Length of component-',len(components))       \n",
    "        \n",
    "        # taking one edge at a time to remove              \n",
    "        edge_to_remove = eb[i][0]\n",
    "        #print('Highest approximate_betweenness-',edge_to_remove)\n",
    "               \n",
    "        G.remove_edge(*edge_to_remove)        \n",
    "        i = i + 1     #eb.pop(edge_to_remove, None)\n",
    "        \n",
    "        # checking 2 components created after removal of edge or not\n",
    "        components = [c for c in nx.connected_component_subgraphs(G)]\n",
    "        \n",
    "        #print('2.Length of component-',len(components))\n",
    "        #print('2.Number of Edges-',len(G.edges()))\n",
    "\n",
    "    result = [c for c in components]\n",
    "    #print('components=',result)\n",
    "\n",
    "    #print('%d----------->Out partition_girvan_newman'%(max_depth))\n",
    "    return (result)\n",
    "\n",
    "    \n",
    "    pass\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subgraph(graph, min_degree):\n",
    "    \"\"\"Return a subgraph containing nodes whose degree is\n",
    "    greater than or equal to min_degree.\n",
    "    We'll use this in the main method to prune the original graph.\n",
    "\n",
    "    Params:\n",
    "      graph........a networkx graph\n",
    "      min_degree...degree threshold\n",
    "    Returns:\n",
    "      a networkx graph, filtered as defined above.\n",
    "\n",
    "    >>> subgraph = get_subgraph(example_graph(), 3)\n",
    "    >>> sorted(subgraph.nodes())\n",
    "    ['B', 'D', 'F']\n",
    "    >>> len(subgraph.edges())\n",
    "    2\n",
    "\n",
    "\n",
    "    >>> subgraph = get_subgraph(example_graph(), 4)\n",
    "    >>> sorted(subgraph.nodes())\n",
    "    ['D']\n",
    "    >>> len(subgraph.edges())\n",
    "    0\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    ###TODO\n",
    "    #print('-------------> In get_subgraph')\n",
    "    #print('Number of edges=',len(graph.edges()))\n",
    "\n",
    "    deg = graph.degree()\n",
    "    \n",
    "    to_remove = [n for n in deg if (deg[n] < min_degree) ]\n",
    "    \n",
    "    graph.remove_nodes_from(to_remove)  \n",
    "\n",
    "\n",
    "    #print('Number of edges=',len(graph.edges()))\n",
    "\n",
    "    #print('-------------> Out get_subgraph')\n",
    "    return(graph)\n",
    "    \n",
    "    pass\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"\n",
    "Compute the normalized cut for each discovered cluster.\n",
    "I've broken this down into the three next methods.\n",
    "\"\"\"\n",
    "\n",
    "def volume(nodes, graph):\n",
    "    \"\"\"\n",
    "    Compute the volume for a list of nodes, which\n",
    "    is the number of edges in `graph` with at least one end in\n",
    "    nodes.\n",
    "    Params:\n",
    "      nodes...a list of strings for the nodes to compute the volume of.\n",
    "      graph...a networkx graph\n",
    "\n",
    "    >>> volume(['A', 'B', 'C'], example_graph())\n",
    "    4\n",
    "\n",
    "    \n",
    "    >>> volume(['B'], example_graph())\n",
    "    3\n",
    "    \n",
    "    >>> volume(['D','E', 'F', 'G'], example_graph())\n",
    "    6\n",
    "\n",
    "    >>> volume(['B', 'D', 'G'], example_graph())\n",
    "    7\n",
    "    \n",
    "    >>> volume(['F', 'G'], example_graph())\n",
    "    4\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    ###TODO\n",
    "       \n",
    "    #print('---------> In volume')\n",
    "   \n",
    "    G = graph.copy()\n",
    "    initial_num_of_edges = graph.number_of_edges()\n",
    "    \n",
    "    G.remove_nodes_from(nodes)   #***********\n",
    "   \n",
    "    num_of_edges = G.number_of_edges()\n",
    "  \n",
    "    link_counter = initial_num_of_edges - num_of_edges\n",
    "    \n",
    "    #print('link_counter=',link_counter)\n",
    "    #print('initial_num_of_edges=',initial_num_of_edges)\n",
    "    #print('num_of_edges=',num_of_edges)\n",
    "    \n",
    "    #print('---------> Out volume')\n",
    "    return(link_counter)\n",
    "      \n",
    "    pass\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut(S, T, graph):\n",
    "    \"\"\"\n",
    "    Compute the cut-set of the cut (S,T), which is\n",
    "    the set of edges that have one endpoint in S and\n",
    "    the other in T.\n",
    "    Params:\n",
    "      S.......set of nodes in first subset\n",
    "      T.......set of nodes in second subset\n",
    "      graph...networkx graph\n",
    "    Returns:\n",
    "      An int representing the cut-set.\n",
    "\n",
    "    >>> cut(['A', 'B', 'C'], ['D', 'E', 'F', 'G'], example_graph())\n",
    "    1\n",
    "    \n",
    "    >>> cut(['F', 'G'], ['A', 'B', 'C', 'D', 'E'], example_graph())\n",
    "    3\n",
    "        \n",
    "    >>> cut(['E', 'F'], ['A', 'B', 'C', 'D', 'G'], example_graph())\n",
    "    3\n",
    "    \n",
    "    >>> cut(['E'], ['A', 'B', 'C', 'D', 'F', 'G'], example_graph())\n",
    "    2\n",
    " \n",
    " \n",
    "    \"\"\"\n",
    "    ###TODO\n",
    "    \n",
    "    #print('------------->In cut')      \n",
    "   \n",
    "    G1 = graph.copy()\n",
    "    G2 = graph.copy()\n",
    "    initial_num_of_edges = graph.number_of_edges()\n",
    "    #print('initial_num_of_edges=',initial_num_of_edges)    \n",
    "   \n",
    "    G1.remove_nodes_from(S)  \n",
    "    G1_edges = G1.number_of_edges()\n",
    "    #print('S=',len(G1.edges()))\n",
    "    \n",
    "    G2.remove_nodes_from(T)\n",
    "    G2_edges = G2.number_of_edges()  \n",
    "    #print('T=',len(G2.edges())) \n",
    "         \n",
    "    link_counter = initial_num_of_edges - (G1_edges + G2_edges)\n",
    "    #print('cut_Size=',link_counter) \n",
    "    \n",
    "    \n",
    "    #print('------------->Out cut')   \n",
    "    return(link_counter)\n",
    "    \n",
    "    pass\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_cut(S, T, graph):\n",
    "    \"\"\"\n",
    "    The normalized cut value for the cut S/T. (See lec06.)\n",
    "    Params:\n",
    "      S.......set of nodes in first subset\n",
    "      T.......set of nodes in second subset\n",
    "      graph...networkx graph\n",
    "    Returns:\n",
    "      An float representing the normalized cut value\n",
    "\n",
    "\n",
    "    >>> norm_cut(['A', 'B', 'C'], ['D', 'E', 'F', 'G'], example_graph())\n",
    "    0.41666666666666663\n",
    "\n",
    "    >>> norm_cut(['F', 'G'], ['A', 'B', 'C', 'D', 'E'], example_graph())\n",
    "    1.125\n",
    "\n",
    "    \"\"\"\n",
    "    ###TODO\n",
    "    \n",
    "    #print('------------->In norm_cut')   \n",
    "    volume1 = volume(S,graph) # 1st set\n",
    "    volume2 = volume(T,graph)   # 2nd set \n",
    "    cut_size = cut(S, T, graph) # cut size\n",
    "    \n",
    "   \n",
    "    if(volume1 == 0):\n",
    "        part1 = 0.0\n",
    "    else :\n",
    "        part1 = 1. * (cut_size/volume1)\n",
    "    \n",
    "    if(volume2 == 0):\n",
    "        part2 = 0.\n",
    "    else : \n",
    "        part2 = 1. * (cut_size/volume2)\n",
    " \n",
    "    norm_value = part1 + part2\n",
    "    \n",
    "\n",
    "    #print('------------->Out norm_cut')\n",
    "    #print('Norm_value = ',norm_value)\n",
    "    return(norm_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_max_depths(graph, max_depths):\n",
    "    \"\"\"\n",
    "    In order to assess the quality of the approximate partitioning method\n",
    "    we've developed, we will run it with different values for max_depth\n",
    "    and see how it affects the norm_cut score of the resulting partitions.\n",
    "    Recall that smaller norm_cut scores correspond to better partitions.\n",
    "\n",
    "    Params:\n",
    "      graph........a networkx Graph\n",
    "      max_depths...a list of ints for the max_depth values to be passed\n",
    "                   to calls to partition_girvan_newman\n",
    "\n",
    "    Returns:\n",
    "      A list of (int, float) tuples representing the max_depth and the\n",
    "      norm_cut value obtained by the partitions returned by\n",
    "      partition_girvan_newman. See Log.txt for an example.\n",
    "\n",
    " \n",
    "\n",
    "      \n",
    "    \"\"\"\n",
    "    ###TODO\n",
    "    #print('------------->In score_max_depths')\n",
    "    \n",
    "    list1 = [] # keeping results of norm_value\n",
    "    \n",
    "\n",
    "    for depth in max_depths:\n",
    "        #print('For MAX_depth =',depth)\n",
    "        # 1st call approximate partitioning method\n",
    "        components = partition_girvan_newman(graph, depth)\n",
    "    \n",
    "        # 2nd call norm_cut  \n",
    "     \n",
    "        norm_value = norm_cut(components[0].nodes(), components[1].nodes(), graph)\n",
    "        \n",
    "        #storing norm_values to list\n",
    "        list1.append([depth,norm_value])\n",
    "           \n",
    "        # see whether smaller norm_cut scores correspond to better partitions \n",
    "  \n",
    "    #print('------------->Out score_max_depths')\n",
    "    #print('List of Norm-value = ',list1)\n",
    "    return(list1)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Link prediction\n",
    "\n",
    "# Next, we'll consider the link prediction problem. In particular,\n",
    "# we will remove 5 of the accounts that Bill Gates likes and\n",
    "# compute our accuracy at recovering those links.\n",
    "\n",
    "def make_training_graph(graph, test_node, n):\n",
    "    \"\"\"\n",
    "    To make a training graph, we need to remove n edges from the graph.\n",
    "    As in lecture, we'll assume there is a test_node for which we will\n",
    "    remove some edges. Remove the edges to the first n neighbors of\n",
    "    test_node, where the neighbors are sorted alphabetically.\n",
    "    E.g., if 'A' has neighbors 'B' and 'C', and n=1, then the edge\n",
    "    ('A', 'B') will be removed.\n",
    "\n",
    "    Be sure to *copy* the input graph prior to removing edges.\n",
    "\n",
    "    Params:\n",
    "      graph.......a networkx Graph\n",
    "      test_node...a string representing one node in the graph whose\n",
    "                  edges will be removed.\n",
    "      n...........the number of edges to remove.\n",
    "\n",
    "    Returns:\n",
    "      A *new* networkx Graph with n edges removed.\n",
    "\n",
    "    In this doctest, we remove edges for two friends of D:\n",
    "\n",
    "\n",
    "   In this doctest, we remove edges for two friends of D:\n",
    "=======\n",
    "\n",
    "    >>> g = example_graph()\n",
    "    >>> sorted(g.neighbors('D'))\n",
    "    ['B', 'E', 'F', 'G']\n",
    "    >>> train_graph = make_training_graph(g, 'D', 2)\n",
    "    >>> sorted(train_graph.neighbors('D'))\n",
    "    ['F', 'G']\n",
    "\n",
    "    \n",
    "    >>> g = example_graph()\n",
    "    >>> sorted(g.neighbors('B'))\n",
    "    ['A', 'C', 'D']\n",
    "    >>> train_graph = make_training_graph(g, 'B', 2)\n",
    "    >>> sorted(train_graph.neighbors('B'))\n",
    "    ['D']\n",
    " \n",
    "    >>> g = example_graph()\n",
    "    >>> sorted(g.neighbors('F'))\n",
    "    ['D', 'E', 'G']\n",
    "    >>> train_graph = make_training_graph(g, 'F', 2)\n",
    "    >>> sorted(train_graph.neighbors('F'))\n",
    "    ['G']\n",
    "\n",
    "    >>> g = example_graph()\n",
    "    >>> sorted(g.neighbors('B'))\n",
    "    ['A', 'C', 'D']\n",
    "    >>> train_graph = make_training_graph(g, 'B', 1)\n",
    "    >>> sorted(train_graph.neighbors('B'))\n",
    "    ['C', 'D']\n",
    "    \n",
    "    >>> g = example_graph()\n",
    "    >>> sorted(g.neighbors('G'))\n",
    "    ['D', 'F']\n",
    "    >>> train_graph = make_training_graph(g, 'G', 1)\n",
    "    >>> sorted(train_graph.neighbors('G'))\n",
    "    ['F']\n",
    "  \n",
    "    \n",
    "    \"\"\"\n",
    "    ###TODO\n",
    "    #print('------------->In make_training_graph')\n",
    "    \n",
    "    G = graph.copy() \n",
    "    num_of_cut = 0\n",
    "    neighbours= sorted(G.neighbors(test_node))[:n] # first n sorted neighbors\n",
    "  \n",
    "   \n",
    "    while (num_of_cut < n) :\n",
    "       \n",
    "       nbr = neighbours[num_of_cut] # taking neighbor one by one\n",
    "       G.remove_edge(test_node,nbr)\n",
    "       num_of_cut = num_of_cut + 1\n",
    "\n",
    "\n",
    "    #print('-------------> Out make_training_graph')\n",
    "    return(G)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard(graph, node, k):\n",
    "    \"\"\"\n",
    "    Compute the k highest scoring edges to add to this node based on\n",
    "    the Jaccard similarity measure.\n",
    "    Note that we don't return scores for edges that already appear in the graph.\n",
    "\n",
    "    Params:\n",
    "      graph....a networkx graph\n",
    "      node.....a node in the graph (a string) to recommend links for.\n",
    "      k........the number of links to recommend.\n",
    "\n",
    "    Returns:\n",
    "      A list of tuples in descending order of score representing the\n",
    "      recommended new edges. Ties are broken by\n",
    "      alphabetical order of the terminal node in the edge.\n",
    "\n",
    "    In this example below, we remove edges (D, B) and (D, E) from the\n",
    "    example graph. The top two edges to add according to Jaccard are\n",
    "    (D, E), with score 0.5, and (D, A), with score 0. (Note that all the\n",
    "    other remaining edges have score 0, but 'A' is first alphabetically.)\n",
    "\n",
    "    >>> g = example_graph()\n",
    "    >>> train_graph = make_training_graph(g, 'D', 2)\n",
    "    >>> jaccard(train_graph, 'D', 2)\n",
    "    [(('D', 'E'), 0.5), (('D', 'A'), 0.0)]\n",
    "\n",
    "    \n",
    "    >>> g = example_graph()\n",
    "    >>> train_graph = make_training_graph(g, 'B', 2)\n",
    "    >>> jaccard(train_graph, 'B', 3)\n",
    "    [(('B', 'E'), 0.5), (('B', 'G'), 0.5), (('B', 'F'), 0.3333333333333333)]\n",
    "\n",
    "    >>> g = example_graph()\n",
    "    >>> train_graph = make_training_graph(g, 'B', 1)\n",
    "    >>> jaccard(train_graph, 'B', 3)\n",
    "    [(('B', 'A'), 0.5), (('B', 'E'), 0.3333333333333333), (('B', 'G'), 0.3333333333333333)]\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    ###TODO\n",
    "    \n",
    "    \n",
    "    neighbors = set(graph.neighbors(node)) #first set\n",
    "    \n",
    "\n",
    "    scores = [] # keeping all scores\n",
    "    \n",
    "    FinalScores = [] # keeping only k highest scores\n",
    "    \n",
    "    #calcualting scores\n",
    "    for node1 in graph.nodes():\n",
    "      if (node1 != node)  :\n",
    "        neighbors2 = set(graph.neighbors(node1)) #2nd set\n",
    "      \n",
    "        value = (1. * (len(neighbors & neighbors2) / len(neighbors | neighbors2)))\n",
    "             \n",
    "        scores.append(((node,node1),value))\n",
    "      \n",
    "    \n",
    "    # removing the edges present in the graphs from scores list\n",
    "    for edge in graph.edges() :      \n",
    "        node3 = edge[0]\n",
    "        node4 = edge[1]\n",
    "        for edge_tpl in scores :\n",
    "           node1 = edge_tpl[0][0]\n",
    "           node2 = edge_tpl[0][1]\n",
    "           if ((node1 == node3 and node2 == node4 ) or (node1 == node4 and node2 == node3 ))  :                                            \n",
    "               scores.remove(edge_tpl)\n",
    "    \n",
    "            \n",
    "    scores.sort(key=lambda x: (-x[1],x[0][0],x[0][1]))\n",
    "\n",
    "    FinalScores = scores[:k] # after sorting just taking first k number of links to recommend.\n",
    "                      \n",
    "    return (FinalScores)\n",
    "\n",
    "    \n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One limitation of Jaccard is that it only has non-zero values for nodes two hops away.\n",
    "#\n",
    "# Implement a new link prediction function that computes the similarity between two nodes $x$ and $y$  as follows:\n",
    "#\n",
    "# $$\n",
    "# s(x,y) = \\beta^i n_{x,y,i}\n",
    "# $$\n",
    "#\n",
    "# where\n",
    "# - $\\beta \\in [0,1]$ is a user-provided parameter\n",
    "# - $i$ is the length of the shortest path from $x$ to $y$\n",
    "# - $n_{x,y,i}$ is the number of shortest paths between $x$ and $y$ with length $i$\n",
    "\n",
    "\n",
    "def path_score(graph, root, k, beta):\n",
    "    \"\"\"\n",
    "    Compute a new link prediction scoring function based on the shortest\n",
    "    paths between two nodes, as defined above.\n",
    "\n",
    "    Note that we don't return scores for edges that already appear in the graph.\n",
    "\n",
    "    This algorithm should have the same time complexity as bfs above.\n",
    "\n",
    "    Params:\n",
    "      graph....a networkx graph\n",
    "      root.....a node in the graph (a string) to recommend links for.\n",
    "      k........the number of links to recommend.\n",
    "      beta.....the beta parameter in the equation above.\n",
    "\n",
    "    Returns:\n",
    "      A list of tuples in descending order of score. Ties are broken by\n",
    "      alphabetical order of the terminal node in the edge.\n",
    "\n",
    "    In this example below, we remove edge (D, F) from the\n",
    "    example graph. The top two edges to add according to path_score are\n",
    "    (D, F), with score 0.5, and (D, A), with score .25. (Note that (D, C)\n",
    "    is tied with a score of .25, but (D, A) is first alphabetically.)\n",
    "\n",
    "    >>> g = example_graph()\n",
    "    >>> train_graph = g.copy()\n",
    "    >>> train_graph.remove_edge(*('D', 'F'))\n",
    "    >>> path_score(train_graph, 'D', k=4, beta=.5)\n",
    "    [(('D', 'F'), 0.5), (('D', 'A'), 0.25), (('D', 'C'), 0.25)]\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    ###TODO\n",
    "    #print(graph.edges())\n",
    "    #print(root)\n",
    "    #print(k)\n",
    "    #print(beta)\n",
    "    \n",
    "    S_list = []    # keeping all scores S(x,y)\n",
    "    node1 = root  #******\n",
    "    \n",
    "    # used same bfs function just ignored unwanted part here\n",
    "    # so same complexity as bfs as required\n",
    "    def search_paths(Graph, start ,end):\n",
    "      \n",
    "      number_of_paths  = 0  # storing number of shortest paths\n",
    "      short_length = 0      # shortest path length\n",
    "      node2distances = dict() \n",
    "      node2num_paths = dict()\n",
    "\n",
    "    \n",
    "      d = deque()\n",
    "      node2distances[start]=0\n",
    "      node2num_paths[start]=1\n",
    "      \n",
    "      d.append(start)\n",
    "      \n",
    "      while (len(d) >= 1) :\n",
    " \n",
    "          current = d.popleft()                                      \n",
    "          nbr_list = graph.neighbors(current)\n",
    "                                             \n",
    "          for nbr in nbr_list :\n",
    "              \n",
    "              if nbr not in node2distances.keys():                        \n",
    "                 node2distances[nbr] = node2distances[current] + 1  \n",
    "                 d.append(nbr)                   \n",
    "                                \n",
    "              if nbr not in node2num_paths.keys() :\n",
    "                  node2num_paths[nbr] = node2num_paths[current]\n",
    "              else :\n",
    "                  if(node2distances[nbr] > node2distances[current]) :    \n",
    "                     node2num_paths[nbr] += node2num_paths[current]\n",
    "                     \n",
    "             \n",
    "                               \n",
    "      short_length =  node2distances[end]\n",
    "      number_of_paths = node2num_paths[end]                            \n",
    "      #print('node2parents',node2parents) \n",
    "      #print('node2distances',node2distances) \n",
    "      #print('node2num_paths',node2num_paths) \n",
    "                   \n",
    "      return(short_length,number_of_paths)  \n",
    "                 \n",
    "      \n",
    "    for node2 in graph.nodes() :           \n",
    "         if (node1,node2) not in graph.edges()  and (node1 != node2) and (node2,node1) not in graph.edges() :\n",
    "                \n",
    "                 #print('(node1,node2)=(%s,%s)'%(node1,node2))\n",
    "                                  \n",
    "                 short_length,number_of_paths = search_paths(graph,node1,node2)  # calling bfs function to get short path length and number of short paths                \n",
    "                 \n",
    "                 #print('short_length ---->=',short_length)\n",
    "\n",
    "                 #print('number_of_paths---->=', number_of_paths)\n",
    "                 \n",
    "                 value1 = pow(beta,short_length)#(beta ** short_length) \n",
    "                 value2 = number_of_paths\n",
    "                 \n",
    "                 value = value1 * value2\n",
    "                   \n",
    "                 S_list.append(((node1,node2),value))\n",
    "                 \n",
    "                 #print('Value1=',value1)\n",
    "                 #print('Value2=',value2)\n",
    "                 #print('Score Value=',value)\n",
    "        \n",
    "    S_list.sort(key=lambda x: (-x[1],x[0][0],x[0][1]))\n",
    "\n",
    "    FinalScores = S_list[:k]   # after sorting just taking first k number of links to recommend.  \n",
    "    \n",
    "    return(FinalScores)\n",
    "    pass\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(predicted_edges, graph):\n",
    "    \"\"\"\n",
    "    Return the fraction of the predicted edges that exist in the graph.\n",
    "\n",
    "    Args:\n",
    "      predicted_edges...a list of edges (tuples) that are predicted to\n",
    "                        exist in this graph\n",
    "      graph.............a networkx Graph\n",
    "\n",
    "    Returns:\n",
    "      The fraction of edges in predicted_edges that exist in the graph.\n",
    "\n",
    "    In this doctest, the edge ('D', 'E') appears in the example_graph,\n",
    "    but ('D', 'A') does not, so 1/2 = 0.5\n",
    "\n",
    "    >>> evaluate([('D', 'E'), ('D', 'A')], example_graph())\n",
    "    0.5\n",
    "\n",
    "    \n",
    "    >>> evaluate([('D', 'E'), ('D', 'A'), ('F', 'G')], example_graph())\n",
    "    0.6666666666666666\n",
    "    \n",
    "    >>> evaluate([('D', 'E')], example_graph())\n",
    "    1.0\n",
    "\n",
    "    >>> evaluate([('D', 'E'), ('D', 'A'), ('F', 'G'), ('D', 'F')], example_graph())\n",
    "    0.75\n",
    "  \n",
    "    \n",
    "    \"\"\"\n",
    "    ###TODO\n",
    "  \n",
    "    size = len(predicted_edges) # size of predicted_edges\n",
    "      \n",
    "    presence = 0 # to calculate number of edges present in graph\n",
    "        \n",
    "    for edge1,edge2 in predicted_edges :           \n",
    "        if graph.has_edge(edge1,edge2) :\n",
    "                  presence = presence + 1\n",
    "                 \n",
    "                  \n",
    "    fraction = 1. * (presence/size)\n",
    "\n",
    "    return(fraction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def download_data():\n",
    "    \"\"\"\n",
    "    Download the data. Done for you.\n",
    "    \"\"\"\n",
    "    urllib.request.urlretrieve('http://cs.iit.edu/~culotta/cs579/a1/edges.txt.gz', 'edges.txt.gz')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_graph():\n",
    "    \"\"\" Read 'edges.txt.gz' into a networkx **undirected** graph.\n",
    "    Done for you.\n",
    "    Returns:\n",
    "      A networkx undirected graph.\n",
    "    \"\"\"\n",
    "    return nx.read_edgelist('edges.txt.gz', delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    FYI: This takes ~10-15 seconds to run on my laptop.\n",
    "    \"\"\"\n",
    "    download_data()\n",
    "    graph = read_graph()\n",
    "    print('graph has %d nodes and %d edges' %\n",
    "          (graph.order(), graph.number_of_edges()))\n",
    "    subgraph = get_subgraph(graph, 2)\n",
    "    print('subgraph has %d nodes and %d edges' %\n",
    "          (subgraph.order(), subgraph.number_of_edges()))\n",
    "    print('norm_cut scores by max_depth:')\n",
    "    print(score_max_depths(subgraph, range(1,5)))\n",
    "    clusters = partition_girvan_newman(subgraph, 3)\n",
    "    print('first partition: cluster 1 has %d nodes and cluster 2 has %d nodes' %\n",
    "          (clusters[0].order(), clusters[1].order()))\n",
    "    print('cluster 2 nodes:')\n",
    "    print(clusters[1].nodes())\n",
    "\n",
    "    test_node = 'Bill Gates'\n",
    "    train_graph = make_training_graph(subgraph, test_node, 5)\n",
    "    print('train_graph has %d nodes and %d edges' %\n",
    "          (train_graph.order(), train_graph.number_of_edges()))\n",
    "\n",
    "\n",
    "    jaccard_scores = jaccard(train_graph, test_node, 5)\n",
    "    print('\\ntop jaccard scores for Bill Gates:')\n",
    "    print(jaccard_scores)\n",
    "    print('jaccard accuracy=%g' %\n",
    "          evaluate([x[0] for x in jaccard_scores], subgraph))\n",
    "\n",
    "    path_scores = path_score(train_graph, test_node, k=5, beta=.1)\n",
    "    print('\\ntop path scores for Bill Gates for beta=.1:')\n",
    "    print(path_scores)\n",
    "    print('path accuracy for beta .1=%g' %\n",
    "          evaluate([x[0] for x in path_scores], subgraph))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph has 5062 nodes and 6060 edges\n",
      "subgraph has 712 nodes and 1710 edges\n",
      "norm_cut scores by max_depth:\n",
      "[[1, 1.0070175438596491], [2, 1.0005847953216374], [3, 0.12177725118483412], [4, 0.12177725118483412]]\n",
      "first partition: cluster 1 has 11 nodes and cluster 2 has 701 nodes\n",
      "cluster 2 nodes:\n",
      "['USAID - US Agency for International Development', 'Smash', 'Sandra Rotman Centre', 'A Plus', 'Ripple Effect Images', 'TEDx', 'Fast Company', 'Xbox', 'Family Planning 2020', 'Endangered Species Protection - RAGES', 'Being Liberal', 'Concern Worldwide', 'يونيسف – اليمن UNICEF Yemen', 'P&G', 'The Square', 'The Insatiable Traveler', 'Momentum1000', 'Facebook in Education', 'Stomp Out Malaria', 'Shot@Life', 'Maternity Worldwide', 'The Metropolitan Museum of Art, New York', 'World Bank', 'Christiane Amanpour', 'Spotify', 'Microsoft', 'National Geographic Education', 'The Onion', 'North American Power', 'Doctors Without Borders/ Médecins Sans Frontières (MSF)', 'Pau Gasol', 'Barbara Lee', 'International Trachoma Initiative', 'UNICEF Central African Republic', '(RED)', 'Human Rights Watch', 'Every Mother Counts', 'Friends of the Global Fund Africa', 'Magnum Photos', 'Unicef Rwanda', 'Abdul Latif Jameel Poverty Action Lab (J-PAL)', 'Non-Profits on Facebook', 'UNFCCC', 'UNICEF Albania', 'Dexter', 'Roll Back Malaria', 'Greenpeace USA', 'Forbes', 'Microsoft Tag', 'Seattle Foundation', 'United Nations Human Rights', 'UNESCO', 'Room to Read: World Change Starts with Educated Children (Official)', 'United Nations Photo', 'The Verge', 'International Initiative for Impact Evaluation (3ie)', 'TIME', 'Chipotle Mexican Grill', 'TEDxChange', 'National History Day California', 'Global Shapers', 'Global Shapers Monterrey Hub', 'Longreads', 'AGRA Alliance', 'Reagan Library', 'African Union', 'National Council for the Social Studies', 'Grand Challenges Canada', 'TIME Photo', 'Global Alliance for Clean Cookstoves', 'The Case Foundation', 'Kickstarter', 'The Commonwealth', 'DefeatDD', 'HISTORY', 'International Space Station', 'John Green', 'Clinton Global Initiative', 'Sesame Workshop', 'Grameen Foundation', 'Kula Project', 'Syngenta', 'Vaccines Today', 'MTV Issues', 'Global Entrepreneurship Week', 'MADE in Europe', 'BBC Newsnight', 'Coca-Cola', 'Rotaract Club Barcelona', 'Facebook', 'Social Good', 'Surface', 'Nelson Mandela', 'The Georgia Geographic Alliance', 'The Brookings Institution', 'ShelterBox', 'GroupMe', 'Facebook Games', 'PBS', 'Instagram', \"Department of State -- Office of Global Women's Issues\", 'Journalists on Facebook', 'MythBusters', 'Jackson Heights Rotary Club', 'EA SPORTS', 'Willie Nelson', 'Qatar Foundation', 'The Rachel Maddow Show', 'Clinton Bush Haiti Fund', 'EA SPORTS FIFA', 'USAID for Global Health', 'UNAIDS', 'Outlook', 'Kentucky Council for the Social Studies', 'A Path Appears', 'Food and Agriculture Organization of the United Nations (FAO)', 'Random Acts Revolution', 'PGA TOUR Golf Challenge', 'Women in the World', 'Mom Bloggers for Social Good', 'US National Archives', 'Enough Food IF', 'Young Global Leaders', 'PBL for Teachers', 'SK - Sports India', 'UNICEF Africa', 'BlogHer', 'Ashoka', 'African Green Revolution Forum', 'World Food Programme', 'Kid President', 'Clinton Foundation', 'Guardian global development', 'Committee for Children', 'Rotary Club of Wall Street New York', 'Education Week', 'The MasterCard Foundation', \"Women's Policy, Inc.\", 'UN-Water World Water Day', 'FIFA Ultimate Team', 'U.S. Senator Lindsey Graham', 'UN Women Asia and the Pacific', 'KA Lite', 'High Tech Teachers', 'The National Press Club', 'SOTENI International', 'Amazon Smile', 'Colorado Geographic Alliance', 'GE', 'Peace Corps', 'The Wellbeing Foundation', 'CityClub Seattle', 'Massachusetts Council for the Social Studies', 'CNN', 'TechCrunch', 'Charlie Rose', 'Quartz', 'Panasonic', 'One Day On Earth', 'Global Fund Advocates Network', 'Women Deliver', 'Oklahoma Council for the Social Studies', 'This American Life', 'United Nations in Belarus', 'National Science Teachers Association', '公益財団法人日本ユニセフ協会', 'Ellen DeGeneres', 'U.S. Global Leadership Coalition (USGLC)', 'United Nations OCHA', 'Richard Branson', 'Slate.com', 'ASUS', 'Rotary Peace Centers', 'Global Shapers Community - Quito, Ecuador Hub', 'ProPublica', 'UN Radio', 'StopBullying.Gov', 'Aeon Magazine', 'The Olympic Games', 'The New York Times', 'California Charter Schools Association', 'U.S. Department of State:  Bureau of Population, Refugees, and Migration', 'WOW at Southbank Centre', 'Global Accelerator', 'UNICEF Guinea', 'TED-Ed', 'UNFPA', 'Devex', 'Entertainment Weekly', 'UNHCR', 'Civicus: World Alliance for Citizen Participation', 'On Point Radio', 'San Joaquin Valley Council for the Social Studies (SJVCSS)', 'Virgin Produced.', 'Invisible Children', 'United Nations Millennium Campaign', \"International Women's Day\", 'Teaching Tolerance', \"Amy Poehler's Smart Girls\", 'Show of Force: Social Good', 'MY World', 'International Atomic Energy Agency (IAEA)', 'Colorado Council for the Social Studies', 'Internet Explorer', 'Malô', 'United Nations News Centre', 'The Daily Show', 'Katharine McPhee', 'FutureWeWant', 'Burn the Rope', 'MDG Health Envoy', 'The New York Times - The Learning Network', 'Real Simple', 'United Nations Association of New Zealand - UNANZ', 'Canada is Dedicated to International Development - DFATD', 'WWF', 'Google Chrome', 'Target', 'Darren Ornitz Photography', 'WHO Regional Office for Europe', 'Chelsea Clinton', 'Co.Create', 'Girl Up', 'President Bill Clinton', 'WASH United', 'Global Education & Skills Forum', 'Tennessee Council for the Social Studies', 'Overdrive Interactive', 'allAfrica.com', 'HeForShe', 'Bustle', 'Facebook Diversity', 'Elizabeth Glaser Pediatric AIDS Foundation', 'TED', 'Rio+20', 'Save the Children', 'Sustainable Development Goals Fund', 'Stanford Graduate School of Business', 'Levo League', 'Green on Facebook', 'Schwab Foundation for Social Entrepreneurship', 'Rio+20 UN Conference on Sustainable Development', 'AJ+', 'NPR Politics', 'International Justice Mission', 'United Nations Regional Information Centre- UNRIC', 'Ted Turner', 'OCSS - The Oregon Council for the Social Studies', 'SJSU Social Science Teacher Preparation', 'INSEAD Knowledge', 'Minnesota Council for the Social Studies', 'Best Buy', 'Windows', 'Al Gore', 'PSY', 'Science', 'TED Live', 'The Ella Rose Collection', 'U.S. Department of Agriculture', 'Co.Exist', 'UNICEF', 'Nicholas Kristof', 'Nokia', 'Children and Youth International', 'Girl Effect', \"Malawi Children's Village\", 'FOX Sports', 'Khan Academy', 'Swiss Malaria Foundation', 'Power the World', 'Qualcomm', 'indieWIRE', 'Unite For Sight', 'Washington Global Health Alliance', 'The Voice', 'Team Gleason', 'University of Washington', 'Women in Public Service', 'Unicef Afghanistan', 'Banque mondiale', 'Greenpeace International', 'Helen Clark', 'NBC', 'YouTube', 'The Bill of Rights Institute', 'Maria Grazia Cucinotta', 'UNICEF Australia', 'NASA - National Aeronautics and Space Administration', 'MSN', 'Greenpeace Rainbow Warrior', 'Bill & Melinda Gates Foundation', 'George Takei', 'Bill & Melinda Gates Foundation Visitor Center', 'Facebook for Business', 'Microsoft Flight', 'Westfield Century City', 'UN Geneva', 'Al Jazeera Channel - قناة الجزيرة الفضائية', 'Bread for the World', 'Upworthy', 'Real Madrid C.F.', 'Rotary Reconnect', 'Kansas Council for the Social Studies', 'Universalist Unitarian Church of Peoria, Illinois', 'Vijender Singh', 'Day of the Girl Summit', 'United Nations Millennium Campaign Africa', 'Senator Patrick Leahy', 'Sustainable Energy for All', 'ActionAid UK', 'GivingTuesday', 'Generosity Day', 'Lalela Project', 'The Global Goals', 'The New Yorker', 'The Lancet', 'BBC News', 'Red Nose Day USA', 'Yvonne Chaka Chaka', 'Microsoft Lumia', 'UNDP Albania', 'U.S. Department of State: Bureau of African Affairs', 'International Fund for Agricultural Development (IFAD)', 'TakePart.com', 'Care2Prevent Auxiliary Board', 'Acumen', 'RESULTS', 'Senator Mark Kirk', 'YouthCare', 'Gerard Piqué', 'UC San Francisco (UCSF)', 'Lutheran Malaria Initiative', \"Shaquille O' Neal\", 'California Geographic Alliance', 'Pike Place Market', 'Pandora', 'Nothing But Nets', 'The Huffington Post', 'UNICEF Iran', 'Nieman Journalism Lab', 'World Animal Protection', 'Solidays', 'Microsoft Excel', 'UNIDO - United Nations Industrial Development Organization', 'UNICEF New Zealand', 'One Billion Rising', 'Discovery', 'Institute for Health Metrics and Evaluation (IHME)', 'Malaria No More', 'Comic Relief: Red Nose Day', 'U.S. Department of State', 'Stop TB Partnership', 'Baker City Rotary Club', 'Save the Children UK', 'Grace Hopper Celebration of Women in Computing', 'Acer', 'The Better World Campaign', 'Better Immunization Data Initiative', 'Rio+Social', 'United Nations in Thailand', 'Rotaract Club of Makati Legaspi', 'myweku.com', 'Megan Hilty Online', 'Stone Oak Rotary Club', 'Chevron', 'Daily Development', 'The Gilder Lehrman Institute of American History', 'Global Health Council', 'Microsoft Research', 'United Nations Assistance Mission in Somalia - UNSOM', 'Fareed Zakaria', 'Aeras', 'Agency', 'Rockefeller Foundation', 'Radio Diaries', 'EA SPORTS PGA TOUR', 'GOOD', 'Big History Project', 'United Nations Visitors Centre', 'State Department- Bureau of International Security and Nonproliferation', 'ONE', 'Foreign Affairs', 'United Nations Development Programme - UNDP', 'Unicef Myanmar', 'End Polio Now', 'PeopleImeet', 'Unicef Polio', 'U.S. Department of State: Engaging the Community on Foreign Affairs', \"Samuel Eto'o\", 'Child Health Now', 'Unicef Somalia', 'Today Show', 'The Cut', 'Social Studies', 'SOIL (Sustainable Organic Integrated Livelihoods)', 'Bill Gates', 'The Global Fund to Fight AIDS, Tuberculosis and Malaria', 'Rotary Italia', 'ViralNova', 'Water.org', 'Ambassador Samantha Power', 'Snap Judgment', 'Andres Iniesta', 'UNITAID', \"The United Nations Girls' Education Initiative -(UNGEI)\", 'Jeffrey Sachs', 'The Tonight Show Starring Jimmy Fallon', 'Malaria No More UK', 'Girls Who Code', 'VSO', 'NBA', 'Facebook Security', 'European Commission - Development & Cooperation - EuropeAid', 'Bradshaw Foundation', 'The Atlantic', 'National History Day', 'Africa Inside Out', 'Eminence', 'Unicef DR Congo', 'Team GB', 'Bring Back Our Girls', 'Mobile Alliance for Maternal Action - MAMA Global', 'One Billion Hungry: Can We Feed the World?', 'World Wildlife Fund', 'University of Maryland, Baltimore', 'We Are Africa United', 'United Against Malaria', 'Microsoft Store', 'Ashton Kutcher', 'Hubble Space Telescope', 'International Federation of Red Cross and Red Crescent Societies', 'UNICEF Haiti', '350.org', 'Panasonic UK', 'Population Services International', 'UN-HABITAT', 'Partners In Health', 'Connect4Climate', 'Malala Fund', 'TEDActive', 'UN Women', 'PBS NewsHour', 'CeeLo Green', 'MediaStorm', 'Evidence Action', 'The White House', \"NPR's Weekend Edition\", 'Microsoft Imagine', 'Global Health and Diplomacy', 'Martha MacCallum', 'CGAP - Consultative Group to Assist the Poor', 'Bond_Free', 'Do Something', 'Catapult', 'UNICEF Nepal', 'Skype', 'Interact Club Illyrian', 'NPR Extra', 'PATH', 'National Geographic Learning', 'United Nations', 'BBC One', 'International Center for Journalists', 'TED Books', 'Global Communities', 'Wheels of Hope', 'Donate My Card', 'WE CARE Solar', 'Facing the Future', 'Sue Desmond-Hellmann', 'STEMconnector', 'The Next Web', 'WNYC Radio', 'Infosys', 'The Global Network for Neglected Tropical Diseases', 'Heifer International', 'UN Sustainable Development Knowledge Platform', 'National Oceanic and Atmospheric Administration (NOAA)', \"Office of the UN Secretary-General's Envoy on Youth\", 'NPR', 'V-Day', 'New Jersey Geographic Alliance', 'Seattle Met Magazine', 'Harvard Business Review', \"Women's Major Group\", 'IRD', '2Pocket Fairtrade', 'Guardian US', 'Governor-General of New Zealand', 'Napkin Labs', 'Shiva Thapa', 'TechWomen', \"Girls' Globe\", 'Congressman Jim Moran', 'TOMS', 'loveLife', 'Nonprofit Organizations', 'The Economist', 'Intel', 'SXSW', 'Jhpiego', 'American Cancer Society', 'Speak Up Africa', 'PATH Drug Development', 'Burke Museum', 'Texas Council for the Social Studies', 'World Health Organization', 'UNICEF Kenya', 'UNEP', 'Clinton Presidential Center', 'Zinduka! Malaria Haikubaliki', 'CGI U', 'Lenovo', 'Mic', 'Lewis Hamilton', 'Facebook Media', 'CDC Global', 'International Committee of the Red Cross', 'UN Women Pacific', 'Vox', 'Global Shapers - San Salvador Hub', 'Ezra Klein', 'OWN: Oprah Winfrey Network', 'CAFOD', 'Nature', 'United Nations Mission in South Sudan (UNMISS)', 'Surge for Water', 'Bing', 'Astronomy Picture of the Day (APOD)', 'Overseas Private Investment Corporation', 'The Malaria Policy Center', 'Simply Measured', 'SumOfUs', 'Jack Wills', 'streetfootballworld', 'Operation ASHA', 'Global Health Strategies', 'Mashable', 'Foro Económico Mundial', 'World Economic Forum - Gender', 'Pulitzer Center on Crisis Reporting', 'D.light Design', 'Radiolab', 'Trees, Water & People', 'TED Radio Hour', 'The Dow Chemical Company', 'Barack Obama', 'Vital Voices', 'Oxfam Ireland', 'Lean In', 'UNMEER', 'The Maternal and Child Survival Program', 'Wisconsin Council for the Social Studies (WCSS)', 'Sylvia A. Earle', 'Run for Polio in Venice', 'Global Poverty Project', 'Sports Illustrated', 'Constitutional Rights Foundation', 'Facebook Developers', 'Intrepid Sea, Air & Space Museum', \"General Federation of Women's Clubs\", 'GBCHealth', 'Emma Watson', 'I fucking love science', 'Scientific American magazine', \"The U.S. President's Emergency Plan for AIDS Relief (PEPFAR)\", 'CNN International', 'Close the Global Gender Gap', 'Digg', 'PAHO-WHO', 'Duke Basketball', 'Tanya Plibersek', 'Duke University', 'UNODC - United Nations Office on Drugs and Crime', 'WIRED', 'Bleacher Report', 'World Health Summit', 'Google', 'Together for Girls', 'UNDP in the Pacific and PNG', 'Cancer', 'World Health Organization (WHO)', 'Entrepreneur', 'National Geographic', 'UNICEF UK', 'The Global Fund for Women', 'Plus Social Good', 'Action/2015', 'Sevenly', 'Global Goals for Sustainable Development', 'BRAC', 'GAVI CSO', 'Derby-Shelton Rotary Club', 'Sasha Alexander', 'Gpange', 'Colonial Williamsburg', 'Xbox Entertainment', 'The Nature Conservancy', 'American Museum of Natural History', 'International Labour Organization (ILO)', 'Gavi, the Vaccine Alliance', 'The New York Times Opinion Section', 'TED.com', 'Skype in the classroom', 'Department of State - Bureau of Democracy, Human Rights and Labor', 'TED Fellows', 'Dell', 'Center for Civic Education', 'Global Shapers - Santo Domingo Hub', 'Georgia Council for the Social Studies (GCSS)', 'Day of the Girl', 'Chase', 'Thrivent Financial', 'Rotary International', 'Comic Relief', 'Global Partnership for Education', 'Unicef Liberia', 'Utah Science Teachers Association', 'Global Citizen', 'Lockheed Martin', 'BlackGirlsCode', 'Desmond Tutu HIV Foundation', 'California Council for the Social Studies', 'World Federation of United Nations Associations', 'HuffPost Impact', 'The Wall Street Journal', 'Beyond Access', 'Gabi Weber', 'UN Peacekeeping', 'charity: water', 'Girls Not Brides', 'United Nations Foundation', 'Rotaract Club of New York at the United Nations', 'Scaling Up Nutrition Movement', 'CARE', 'Cordaid', 'AOL', 'Virgin Galactic', 'Bayer', 'International Partnership for Microbicides', 'The Good Men Project', 'The Late Show with Stephen Colbert', 'General Colin L. Powell', 'Medium', 'Rotary Club of Rancho Bernardo', 'Grassroot Soccer', 'Global Moms Challenge', 'ClickHole', 'The Choices Program', 'United Nations Economic and Social Council', 'Qatar Foundation International', 'The Tony Elumelu Foundation', 'NPR Morning Edition', 'Tom Daley', 'SayNO - UNiTE to End Violence Against Women', 'UNFPA - Burkina Faso', 'Reuters', 'Les Miserables - Musical', 'Overseas Development Institute (ODI)', 'Melinda Gates', 'Sanitation and Water for All', 'Washington Post', 'IRIN', 'Campaign for Australian Aid', 'Results for Development Institute', 'Novartis', 'CES', 'United Nations Commission on the Status of Women (CSW)', 'Bill Nye The Science Guy', 'Global Alliance for Improved Nutrition', 'UNICEF USA', 'MarketingProfs', 'Stanford Social Innovation Review', 'SportsUnited - U.S. Department of State', '10x10 - Girl Rising', 'Top Gear', 'Unicef Burundi', 'Complex Sports', 'TechSoup', 'VillageReach', 'Mother Jones', 'UNICEF Jordan', 'BBC News Magazine', 'UN Web TV', 'Whole Foods Market', 'RESULTS AU', 'NCWIT Aspirations in Computing', 'Global Good Challenge', 'World Economic Forum', 'A Mighty Girl', 'BBC Trending', 'ICDDR,B', 'NRDC (Natural Resources Defense Council)', 'Live Below the Line', 'United Nations Assistance Mission for Iraq (UNAMI)', 'The Independent', 'Virginia Council for the Social Studies', 'EFA Report UNESCO', 'HP', 'Washington State Council for the Social Studies', 'World YWCA']\n",
      "train_graph has 712 nodes and 1705 edges\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "top jaccard scores for Bill Gates:\n",
      "[(('Bill Gates', 'Global Citizen'), 0.16216216216216217), (('Bill Gates', 'Bill & Melinda Gates Foundation'), 0.10344827586206896), (('Bill Gates', 'Grand Challenges Canada'), 0.09375), (('Bill Gates', 'I fucking love science'), 0.09375), (('Bill Gates', 'Girl Effect'), 0.09090909090909091)]\n",
      "jaccard accuracy=0.2\n",
      "\n",
      "top path scores for Bill Gates for beta=.1:\n",
      "[(('Bill Gates', 'Bill & Melinda Gates Foundation'), 0.06000000000000001), (('Bill Gates', 'Global Citizen'), 0.06000000000000001), (('Bill Gates', 'Gavi, the Vaccine Alliance'), 0.04000000000000001), (('Bill Gates', 'FutureWeWant'), 0.030000000000000006), (('Bill Gates', 'Girl Effect'), 0.030000000000000006)]\n",
      "path accuracy for beta .1=0.4\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
