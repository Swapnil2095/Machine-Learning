{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"flower_species_classification_pytorch.ipynb","version":"0.3.2","provenance":[],"private_outputs":true,"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"-EtL_cX3L8DD","colab_type":"code","colab":{}},"cell_type":"code","source":["# This is to set up necessary dependencies, and make sure currenyt versions are installed\n","!pip3 install torch torchvision\n","!pip3 install Pillow\n","\n","# NOTE: it could be needed to restart the runtime after this step if PIL related errors are encountered later during training"],"execution_count":0,"outputs":[]},{"metadata":{"id":"DYvtCCAcjXAr","colab_type":"code","colab":{}},"cell_type":"code","source":["# Download the data\n","!wget https://s3.amazonaws.com/content.udacity-data.com/courses/nd188/flower_data.zip\n","!unzip flower_data.zip\n","\n","# I found it useful and convenient to save the model on Google Drive.\n","# This links the drive to this notebook\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"zhEvL_5iL1Ct","colab_type":"code","colab":{}},"cell_type":"code","source":["# Main imports\n","import numpy as np\n","import time\n","import torch\n","from torchvision import datasets, models, transforms\n","from torch.utils.data.sampler import SubsetRandomSampler\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ffGBgI4uL1DB","colab_type":"code","colab":{}},"cell_type":"code","source":["# Training ad data loader creation, based on the selected pre-trained model\n","def create_loaders(base, final = False):\n","\n","    # Setting up pathnames to training and test data\n","    data_dir = 'flower_data'\n","    train_dir = data_dir + '/train'\n","    valid_dir = data_dir + '/valid'\n","\n","    # creating transforms for pre-processing\n","    # Different pre-trained models expect different img sizes\n","    # ResNet, DenseNet expect 224, Inception expects 299\n","    if base == 'ResNet':\n","        img_size = 224\n","    elif base == 'DenseNet161':\n","        img_size = 224\n","    elif base == 'Inception':\n","        img_size = 299\n","    else:\n","        img_size = 224\n","\n","    transforms_train = transforms.Compose([\n","        transforms.RandomRotation(30),\n","        transforms.RandomResizedCrop(img_size),    # Change this depending on what the model expects as input\n","        transforms.RandomHorizontalFlip(),\n","        transforms.RandomVerticalFlip(),\n","        transforms.ToTensor(),\n","        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n","        ])\n","\n","    transforms_test = transforms.Compose([\n","        transforms.Resize(img_size + 1),           # Change this depending on what the model expects as input\n","        transforms.CenterCrop(img_size),           # Change this depending on what the model expects as input\n","        transforms.ToTensor(),\n","        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n","        ])\n","\n","    # Load the datasets with ImageFolder\n","    datasets_train = datasets.ImageFolder(train_dir, transform=transforms_train)\n","    datasets_test = datasets.ImageFolder(valid_dir, transform=transforms_test)\n","\n","    if final == False:\n","        # Create indeces in training data to split off a portion for validation\n","        valid_size = 0.2\n","        num_train = len(datasets_train)\n","        indices = list(range(num_train))\n","        np.random.shuffle(indices)\n","        split = int(np.floor(valid_size * num_train))\n","        train_idx, valid_idx = indices[split:], indices[:split]\n","\n","        # define samplers for obtaining training and validation batches\n","        train_sampler = SubsetRandomSampler(train_idx)\n","        valid_sampler = SubsetRandomSampler(valid_idx)\n","\n","        # Using the image datasets and the trainforms, define the dataloaders\n","        train_loader = torch.utils.data.DataLoader(datasets_train, batch_size=64, sampler=train_sampler)\n","        valid_loader = torch.utils.data.DataLoader(datasets_train, batch_size=64, sampler=valid_sampler)\n","    else:\n","        train_loader = torch.utils.data.DataLoader(datasets_train, batch_size=64, shuffle = True)\n","        valid_loader = torch.utils.data.DataLoader(datasets_test, batch_size=64, shuffle = True)\n","\n","    # This is used for testing the trained non-final model. Final models are trained on whole train set, using test set for validation\n","    test_loader = torch.utils.data.DataLoader(datasets_test, batch_size=64, shuffle = True)\n","    \n","    return train_loader, valid_loader, test_loader"],"execution_count":0,"outputs":[]},{"metadata":{"id":"X0TK00S7kAIj","colab_type":"code","colab":{}},"cell_type":"code","source":["\"\"\" Defining possible architectures of the final classifier. \n","Instances of these classes will be used to replace the final layer of imported CNN.\n","H2 has 2 hidden layers, H1 has one. Overall, H1 seemed to get better accuracy at this job. \"\"\"\n","\n","class ClassifierH2(nn.Module):\n","    def __init__(self, inp = 2048, h1=1024, h2=512, out = 102, d=0.3):\n","        super().__init__()\n","        self.fc1 = nn.Linear(inp, h1)\n","        self.fc2 = nn.Linear(h1, h2)\n","        self.fc3 = nn.Linear(h2, out)\n","        \n","        self.dropout = nn.Dropout(d)\n","        \n","    def forward(self, x):\n","        x = F.relu(self.fc1(x))\n","        x = self.dropout(x)\n","        x = F.relu(self.fc2(x))\n","        x = self.dropout(x)            \n","        x = self.fc3(x)\n","        \n","        return x\n","      \n","class ClassifierH1(nn.Module):\n","    def __init__(self, inp = 2048, h1=1024, out = 102, d=0.3):\n","        super().__init__()\n","        self.fc1 = nn.Linear(inp, h1)\n","        self.fc2 = nn.Linear(h1, out)\n","        \n","        self.dropout = nn.Dropout(d)\n","        \n","    def forward(self, x):\n","        x = F.relu(self.fc1(x))\n","        x = self.dropout(x)        \n","        x = self.fc2(x)\n","        \n","        return x\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"oyu5Nmin6o9L","colab_type":"code","colab":{}},"cell_type":"code","source":["\"\"\" I build the final model from 2 parts: imported pre-trained CNN - like ResNet - that is used for feature detection, \n","and a customized classifier for final probability scoring. To make experimentation with training easier, I created a special \n","container class that is used for three main things:\n","    - holds reference to imported CNN (with final layer replaced)\n","    - holds reference to custom classifier\n","    - has a number of utility features and methods to help with training and progress tracking \"\"\"\n","\n","class MyNetwork():\n","       \n","    def __init__(self, base, clf, kwargs):\n","        # Additional variables will be used to track training progress for easier re-loads, plotting, etc.\n","        self.last_epoch = 0       # Keeps track of number of epochs trained\n","        self.min_vloss = np.inf   # Keeps track of lowest validation loss\n","        self.lr_hist = []         # Tracks learn rate changes over epochs \n","        self.vloss_hist = []      # Tracks validation loss changes over epochs\n","        self.tloss_hist = []      # Tracks training loss changes over epochs\n","        self.acc_hist = []        # Tracks accuracy rate changes over epochs\n","        self.opt_dict = None      # Stores optimezer dictionary\n","        self.sch_dict = None      # Stores scheduler dictionary\n","        self.sch_wait = None      # Stores scheduler \"patience\" parameter\n","        self.clf_name = clf       # Stores the name of the chosen classifier\n","        self.base_name = base     # Stores the name of chosen CNN\n","        self.loss_name = None     # Stores the name of chosen loss function\n","        self.opt_name = None      # Stores the name of chosen optimizer method\n","        self.layer_dict = kwargs  # Stores the info on number of nodes in each clf layer, as well as dropout %\n"," \n","        # When the datasets are created, each class value automaticaly got assigned an index.\n","        # These will be created and stored in the training function \n","        self.class_to_idx = None\n","        self.idx_to_class = None\n","       \n","        # Create vanilla feature net and classifier\n","        base_net = self.import_model(base)\n","        my_clf = self.create_clf(base, clf, kwargs)\n","\n","        # Adding the classifier to pre-trained network (replacing last layer- this can be named differently for each imported network)\n","        if base == 'ResNet152':\n","            base_net.fc = my_clf\n","        elif base == 'Inception':\n","            base_net.fc = my_clf\n","        elif base == 'DenseNet161':\n","            base_net.classifier = my_clf\n","            \n","        self.my_net = base_net \n","        self.my_clf = my_clf\n","        \n","        print('Model created from base net {}, Clf {} with {}'.format(base, clf, kwargs))\n","        \n","    # Pre-trained conv-net used for feature extraction before final classification\n","    def import_model(self, mod, tr = True):\n","        if mod == 'ResNet152':\n","            m = models.resnet152(pretrained=tr)\n","        elif mod == 'Inception':\n","            m = models.inception_v3(pretrained=tr) # aux_logits=False)\n","        elif mod == 'DenseNet161':\n","            m = models.densenet161(pretrained=tr)\n","        else:\n","            print('No base model imported!')\n","        \n","        # Keep the weights fixed on the pre-trained model\n","        if tr:\n","            for param in m.parameters():\n","                param.requires_grad = False \n","        \n","        return m     \n","        \n","    # Create a specific classifier\n","    def create_clf(self, base, clf, kwargs):\n","      \n","        # Define input size, based on the output of pre-trained models:\n","        if base == 'ResNet152':\n","            in_size = 2048\n","        elif base == 'Inception':\n","            in_size = 2048\n","        elif base == 'DenseNet161':\n","            in_size = 2208\n","      \n","        if clf == 'H1':\n","            return ClassifierH1(in_size, kwargs['h1'], kwargs['out'], kwargs['d'])\n","        elif clf == 'H2':\n","            return ClassifierH2(in_size, kwargs['h1'], kwargs['h2'], kwargs['out'], kwargs['d'])\n","        else:\n","            print('Classifier not found')\n","    \n","    def set_my_params(self, param_dict):\n","        if param_dict['last_epoch']:\n","            self.last_epoch = param_dict['last_epoch']\n","        if param_dict['min_vloss']:\n","            self.min_vloss = param_dict['min_vloss']\n","        if param_dict['lr_hist']:\n","            self.lr_hist = param_dict['lr_hist']\n","        if param_dict['vloss_hist']:\n","            self.vloss_hist = param_dict['vloss_hist']\n","        if param_dict['tloss_hist']:\n","            self.tloss_hist = param_dict['tloss_hist']\n","        if param_dict['acc_hist']:\n","            self.acc_hist = param_dict['acc_hist']\n","        if param_dict['opt_dict']:\n","            self.opt_dict = param_dict['opt_dict']\n","        if param_dict['sch_dict']:\n","            self.sch_dict = param_dict['sch_dict']\n","        if param_dict['sch_wait']:\n","            self.sch_wait = param_dict['sch_wait']\n","        if param_dict['clf_name']:\n","            self.clf_name = param_dict['clf_name']\n","        if param_dict['base_name']:\n","            self.base_name = param_dict['base_name']\n","        if param_dict['loss_name']:\n","            self.loss_name = param_dict['loss_name']\n","        if param_dict['opt_name']:\n","            self.opt_name = param_dict['opt_name']\n","        if param_dict['layer_dict']:\n","            self.layer_dict = param_dict['layer_dict']\n","        if param_dict['class_to_idx']:\n","            self.class_to_idx = param_dict['class_to_idx']\n","        if param_dict['idx_to_class']:\n","            self.idx_to_class = param_dict['idx_to_class']\n","\n","    def get_my_params(self):        \n","        return {\n","                'opt_dict':         self.opt_dict,\n","                'last_epoch':       self.last_epoch,\n","                'min_vloss':        self.min_vloss,\n","                'lr_hist' :         self.lr_hist,\n","                'class_to_idx':     self.class_to_idx,\n","                'idx_to_class':     self.idx_to_class,\n","                'vloss_hist' :      self.vloss_hist,\n","                'tloss_hist' :      self.tloss_hist,\n","                'acc_hist' :        self.acc_hist,\n","                'sch_dict':         self.sch_dict,\n","                'sch_wait':         self.sch_wait,\n","                'clf_name' :        self.clf_name,\n","                'base_name' :       self.base_name,\n","                'loss_name' :       self.loss_name,\n","                'opt_name' :        self.opt_name,\n","                'clf_state_dict':   self.my_clf.state_dict(),\n","                'base_state_dict':  self.my_net.state_dict(), # even though during the training weights of the pre-trained model do not change, some parameters do - like running averages - and those are important to restore for the model to pertform the same after reload\n","                'layer_dict':       self.layer_dict,\n","                'class_to_idx':     self.class_to_idx,\n","                'idx_to_class':     self.idx_to_class\n","                }\n","\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"elMLZU6cKEhf","colab_type":"code","colab":{}},"cell_type":"code","source":["# Helper functions to have flexibility with choice if needed\n","# The optimizer to be used for training\n","def create_optimizer(params, opt, lr, mom = 0.9):\n","    if opt == 'SGD':\n","        return optim.SGD(params, momentum=mom, lr= lr)\n","    elif opt == 'Adam':\n","        return optim.Adam(params, lr= lr)\n","    else:\n","        print('Optimizer not found')\n","  \n","# Scheduler that automatically adjusts learning rate of optimizer\n","def create_scheduler(opt, p=7, f = 0.1):\n","    return optim.lr_scheduler.ReduceLROnPlateau(opt, patience = p, factor=f)\n","\n","# Loss function selector\n","def create_loss(ls):\n","    if ls == 'Entr':\n","        return nn.CrossEntropyLoss()  \n","  "],"execution_count":0,"outputs":[]},{"metadata":{"id":"ylJ9kH2SlYC_","colab_type":"code","colab":{}},"cell_type":"code","source":["# Saving the model parameters\n","def save_model(model, path):\n","  \n","    chp = model.get_my_params()\n","    torch.save(chp, path)\n","\n","# Loading the model.\n","def load_model(path):\n","    \n","    chp = torch.load(path)\n","    m = MyNetwork(chp['base_name'], chp['clf_name'], chp['layer_dict'])  \n","    \n","    # Applying saved parameters\n","    m.set_my_params(chp)\n","    m.my_clf.load_state_dict(chp['clf_state_dict'])\n","    m.my_net.load_state_dict(chp['base_state_dict'])\n","    \n","    print('Model \"{}\" loaded.'.format(path))\n","    print('Last epoch {}, Base net as {}, Clf as {} with {}'.format(m.last_epoch, m.base_name, m.clf_name, m.layer_dict))\n","            \n","    return m\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Q7lDq1jCL1DX","colab_type":"code","colab":{}},"cell_type":"code","source":["\"\"\" This is the main training function. It is based on the training routines provided in Udacity excercise files.\n","It is able to take in a newly created model, as well as resume training from last saved epoch, keeping the latest optimizer\n","and scheduler parameters. Alternatively, it is also possible to \"restart\" learning, by calling this function with\n","a trained model and new set of train_params. This will allow to again train with base learning rate and once again work through the epochs. \"\"\"\n","\n","def train_model(model, n_epochs, isFinal, train_params = None):\n","\n","    # Helper functions for printing oput training progress data\n","    def print_epoch_start_stats(e_start, e_end, current_lr, current_vmin):\n","        \n","        print('*** Epoch [{}/{}]: Training with LR [{:.6f}], current VLoss Min [{:.4f}]'.format(\n","        e_start, e_end, current_lr, current_vmin))\n","\n","    def print_epoch_end_stats(train_loss, valid_loss, valid_acc, epoch_time):\n","        \n","        print('   Train loss: \\t{:.6f}'.format(train_loss))\n","        print('   Valid loss: \\t{:.6f}'.format(valid_loss))\n","        print('   Valid acc: \\t{:.6f}'.format(valid_acc))\n","        print('*** Epoch completed in {:.0f}m {:.0f}s'.format(epoch_time // 60, epoch_time % 60))      \n","\n","    #########################\n","    # Initiate the training #\n","    #########################\n"," \n","    print('*** Training starting ***')\n","    train_start = time.time()  # Track training time\n"," \n","    # Create the data loaders\n","    train_loader, valid_loader, _ = create_loaders(model.base_name, isFinal)\n","    \n","    # Check if it's a new model or an already trained one\n","    if model.last_epoch != 0:\n","        print('Trained model, continuing from last epoch...')\n","    elif (model.last_epoch == 0) and (train_params == None):\n","        raise Exception('Un-trained model, need training params passed!')\n","    else:\n","        print('New model, training from scratch...')\n","        # When the datasets were created, each class value automaticaly got assigned an index.\n","        # Storing tha class-to-index and index-to-class relationships. \n","        model.class_to_idx = train_loader.dataset.class_to_idx\n","        model.idx_to_class = {idx: cl for cl, idx in train_loader.dataset.class_to_idx.items()}\n","\n","    f_epoch = model.last_epoch + 1\n","\n","    # Setting up loss criteria, optimizer and scheduler (to gradualy reduce learn rate).\n","    # If train_params is passed then start with optimizer, loss and lr that are passed as parameters.\n","    # Otherwise use the data saved in model params.\n","    if train_params:\n","        print('Starting training with new optimizer and scheduler...')\n","        criterion = create_loss(train_params['loss'])\n","        optimizer = create_optimizer(model.my_clf.parameters(), train_params['opt'], train_params['lr'])\n","        scheduler = create_scheduler(optimizer, train_params['wait'])\n","        model.loss_name = train_params['loss']\n","        model.opt_name = train_params['opt']\n","        model.sch_wait = train_params['wait']\n","        model.vloss_min = np.inf\n","        \n","        valid_loss_min = np.Inf\n","    else:\n","        print('Starting training with saved optimizer and scheduler...')\n","        criterion = create_loss(model.loss_name)\n","        optimizer = create_optimizer(model.my_clf.parameters(), model.opt_name, model.lr_hist[-1])\n","        optimizer.load_state_dict(model.opt_dict)\n","        scheduler = create_scheduler(optimizer, model.sch_wait)\n","        scheduler.load_state_dict(model.sch_dict)\n","        \n","        valid_loss_min = model.min_vloss\n","    \n","    # check if CUDA is available\n","    train_on_gpu = torch.cuda.is_available()\n","    if not train_on_gpu:\n","        print('CUDA is not available.  Training on CPU ...')\n","        device = \"cpu\"\n","    else:\n","        print('CUDA is available!  Training on GPU ...')\n","        device = \"cuda\"\n","    model.my_net.to(device);    \n","    \n","    #####################\n","    # Go through epochs #\n","    #####################\n","    for epoch in range(f_epoch, n_epochs+f_epoch):\n","        \n","        epoch_start = time.time()\n","        \n","        # get epoch stats to print and print them\n","        end_epoch = n_epochs+f_epoch-1\n","        current_lr = optimizer.state_dict()['param_groups'][0]['lr']       \n","        \n","        print_epoch_start_stats(epoch, end_epoch, current_lr, valid_loss_min)\n","             \n","        # keep track of training and validation loss\n","        train_loss = 0.0\n","        valid_loss = 0.0\n","        valid_acc = 0.0\n","        \n","        ###################\n","        # train the model #\n","        ###################\n","        model.my_net.train()\n","        for data, target in train_loader:\n","            # move tensors to GPU if CUDA is available\n","            if train_on_gpu:\n","                data, target = data.cuda(), target.cuda()\n","            # clear the gradients of all optimized variables\n","            optimizer.zero_grad()\n","            # forward pass: compute predicted outputs by passing inputs to the model\n","            # Special case for inception because in training it has an additional auxiliary output\n","            # In training mode we calculate the loss by summing the final output and the auxiliary output\n","            # but in testing we only consider the final output.\n","            if model.base_name == 'Inception':\n","                # From https://discuss.pytorch.org/t/how-to-optimize-inception-model-with-auxiliary-classifiers/7958\n","                output, aux_output = model.my_net(data)\n","                loss1 = criterion(output, target)\n","                loss2 = criterion(aux_output, target)\n","                loss = loss1 + 0.4*loss2\n","            else:\n","                output = model.my_net(data)\n","                loss = criterion(output, target)\n","            \n","            # backward pass: compute gradient of the loss with respect to model parameters\n","            loss.backward()\n","            # perform a single optimization step (parameter update)\n","            optimizer.step()\n","            # update training loss\n","            train_loss += loss.item()*data.size(0)\n","\n","        ######################    \n","        # validate the model #\n","        ######################\n","        model.my_net.eval()\n","        for data, target in valid_loader:\n","            # move tensors to GPU if CUDA is available\n","            if train_on_gpu:\n","                data, target = data.cuda(), target.cuda()\n","            # forward pass: compute predicted outputs by passing inputs to the model\n","            output = model.my_net(data)\n","            # calculate the batch loss\n","            loss = criterion(output, target)\n","            # update average validation loss \n","            valid_loss += loss.item()*data.size(0)\n","            # calculate accuracy\n","            _, preds = torch.max(output, 1) # get a list of classes with highest probability from each img in the batch\n","            acc = torch.sum(preds == target.data) # Where evaluation condition is true, sum up such cases, e.g. get the number of correct predictions    \n","            valid_acc += acc\n","\n","        # calculate epoch average losses and accuracy\n","        if isFinal:\n","            # Test set is used for validation\n","            train_loss = train_loss / len(train_loader.dataset) \n","            valid_loss = valid_loss / len(valid_loader.dataset)\n","            valid_acc = valid_acc.double() / len(valid_loader.dataset)\n","        else:\n","            # Portion of the training set (sampler) ir used for validation\n","            train_loss = train_loss / len(train_loader.sampler) \n","            valid_loss = valid_loss / len(valid_loader.sampler) \n","            valid_acc = valid_acc.double() / len(valid_loader.sampler)       \n","                \n","        # print training/validation statistics \n","        epoch_time = time.time() - epoch_start\n","        print_epoch_end_stats(train_loss, valid_loss, valid_acc, epoch_time)\n","\n","        # Check if valid loss improved, save if yes\n","        save = False\n","        if valid_loss <= valid_loss_min:\n","            old_loss_min = valid_loss_min\n","            valid_loss_min = valid_loss\n","            save = True\n","\n","        # Store epoch progress\n","        model.last_epoch = epoch\n","        model.min_vloss = valid_loss_min\n","        model.opt_dict = optimizer.state_dict()\n","        model.sch_dict = scheduler.state_dict()\n","        model.lr_hist.append(current_lr)\n","        model.acc_hist.append(valid_acc.item())\n","        model.vloss_hist.append(valid_loss)\n","        model.tloss_hist.append(train_loss)\n","        \n","        # Save model if validation loss has decreased\n","        if save:\n","            print('*** Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n","            old_loss_min, valid_loss_min))\n","            save_model(model, filename_drive_best)\n","            \n","        # Advance scheduler count one step\n","        scheduler.step(valid_loss)\n","        # Epoch ends\n","        \n","    # Training ends\n","    # Save final model for analisys. Best model version already saved based on validation loss \n","    save_model(model, filename_drive_fin)\n","    \n","    # Calculate total training time\n","    total_time = time.time() - train_start\n","    hours = total_time // 3600\n","    minutes = (total_time % 3600) // 60\n","    seconds = ((total_time % 3600) % 60)\n","    print('*** Training complete in {:.0f}h {:.0f}m {:.0f}s ***'.format(\n","        hours, minutes, seconds))\n","\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"J7KcKXIJj0Ef","colab_type":"code","colab":{}},"cell_type":"code","source":["# Function to test the model against the test set. Only usefull when test set ios reserved and not used for validation \n","def test_model(model):\n","    \n","    # Create loss\n","    criterion = create_loss(model.loss_name)\n","    \n","    # Create the data loaders\n","    _, _, test_loader = create_loaders(model.base_name)\n","    \n","    # track test loss\n","    test_loss = 0.0\n","    batch_size = 64\n","    class_correct = list(0. for i in range(len(model.class_to_idx)))\n","    class_total = list(0. for i in range(102))\n","\n","    # check if CUDA is available\n","    test_on_gpu = torch.cuda.is_available()\n","    if not test_on_gpu:\n","        print('CUDA is not available.  Testing on CPU ...')\n","        device = \"cpu\"\n","    else:\n","        print('CUDA is available!  Testing on GPU ...')\n","        device = \"cuda\"\n","    model.my_net.to(device);\n","        \n","    model.my_net.eval()\n","    # iterate over test data\n","    for data, target in test_loader:\n","        # move tensors to GPU if CUDA is available\n","        if test_on_gpu:\n","            data, target = data.cuda(), target.cuda()\n","        # forward pass: compute predicted outputs by passing inputs to the model\n","        output = model.my_net(data)\n","        # calculate the batch loss\n","        loss = criterion(output, target)\n","        # update test loss \n","        test_loss += loss.item()*data.size(0)\n","        # convert output probabilities to predicted class\n","        _, pred = torch.max(output, 1)    \n","        # compare predictions to true label\n","        correct_tensor = pred.eq(target.data.view_as(pred))\n","        correct = np.squeeze(correct_tensor.numpy()) if not test_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n","        # calculate test accuracy for each object class\n","        for i in range(len(target.data)):\n","            label = target.data[i]\n","            class_correct[label] += correct[i].item()\n","            class_total[label] += 1\n","\n","    # average test loss\n","    test_loss = test_loss/len(test_loader.dataset)\n","    print('Test Loss: {:.6f}\\n'.format(test_loss))\n","    \n","    class_accuracies = {}\n","    for i in range(102):\n","        if class_total[i] > 0:\n","            class_accuracy = 100 * class_correct[i] / class_total[i]\n","            print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n","                model.idx_to_class[i], class_accuracy,\n","                np.sum(class_correct[i]), np.sum(class_total[i])))\n","            # Keep the accuracy data for each class\n","            class_accuracies[i+1] = class_accuracy\n","        else:\n","            print('Test Accuracy of %5s: N/A (no training examples)' % (model.idx_to_class[i]))\n","\n","    total_acc = 100. * np.sum(class_correct) / np.sum(class_total)\n","    print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n","        total_acc,\n","        np.sum(class_correct), np.sum(class_total)))\n","\n","\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"wC9mxzwMh6Gv","colab_type":"code","colab":{}},"cell_type":"code","source":["# These are conviniet helper functions to visualize model training stats\n","\n","# Takes a model and plots it's losses, accuracy and learn rates over epochs\n","def plot_model_stats(model):\n","\n","    fig, (ax1, ax2, ax3) = plt.subplots(figsize=(14,6), ncols=3)\n","    \n","    ax1.plot(model.tloss_hist, label='Train loss')\n","    ax1.plot(model.vloss_hist, label='Valid loss')\n","    ax1.legend(frameon=False)\n","    ax1.set_xlabel('Epochs')\n","    \n","    ax2.plot(model.acc_hist, label = 'Valid accuracy')\n","    ax2.legend(frameon=False)\n","    ax2.set_xlabel('Epochs')\n","    \n","    ax3.plot(model.lr_hist, label = 'Learn rate')\n","    ax3.legend(frameon=False)\n","    ax3.set_xlabel('Epochs') \n","    \n","    plt.tight_layout()\n","    \n","# Takes a list of model instances and plots their losses, accuracy and learn rates over epochs   \n","def plot_compare_models(*models):\n","\n","    fig, (ax1, ax2, ax3) = plt.subplots(figsize=(14,6), ncols=3)\n","    \n","    for m in range(len(models)):\n","    \n","        ax1.plot(models[m].vloss_hist, label= 'M{}'.format(m))\n","        ax2.plot(models[m].acc_hist, label = 'M{}'.format(m))\n","        ax3.plot(models[m].lr_hist, label = 'M{}'.format(m))\n","\n","    ax1.legend(frameon=False)\n","    ax1.set_xlabel('Epochs')  \n","    ax1.set_ylabel('Vald Loss') \n","\n","    ax2.legend(frameon=False)\n","    ax2.set_xlabel('Epochs')\n","    ax2.set_ylabel('Accuracy')\n","    \n","    ax3.legend(frameon=False)\n","    ax3.set_xlabel('Epochs')\n","    ax3.set_ylabel('LR history')\n","    \n","    plt.tight_layout()\n","    "],"execution_count":0,"outputs":[]},{"metadata":{"id":"_25q_x_bk4Mt","colab_type":"code","colab":{}},"cell_type":"code","source":["# This cell is for setting the control parameters\n","\n","# File name for saving model\n","filename_drive_best = '/content/gdrive/My Drive/Data/Projects/den_1.pt'\n","filename_drive_fin = '/content/gdrive/My Drive/Data/Projects/den_1_fin.pt'\n","\n","# Defining network model \n","# Base net can be 'Inception', 'ResNet152' or 'DenseNet161', others can be added if needed (in MyNetwork class)\n","# Be sure to check what immage sizes the pre-trained models expect, for example - Inception has 299, while ResNet takes 224 (need to change this in image transforms)\n","# Also make sure that the input size for your custom classifier matches what the original pre-trained model expects on last layer\n","\n","base_net = 'DenseNet161'   \n","clf = 'H1' # H1 has one hidden layer, H2 has two\n","layers = {'out':102, 'd':0.35, 'h1':1024} # Input size is pre-determined and set based on chosen pre-trained model. d is for droput rate.\n","\n","# Training parameters\n","epochs = 60\n","isFinal = True # If set to False, 20% of train set is reserved for validation, and test set reserved for testing the trained model\n","train_params = {\n","                'opt':   'SGD',   # Optimizer to be used for training\n","                'loss':  'Entr',  # Loss function to be used for training\n","                'lr':    0.035,    # Learning rate for training\n","                'wait':  7        # \"patiance\" parameter for scheduler function from Pytorch.\n","                }\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"NAJjiKau3VK8","colab_type":"code","colab":{}},"cell_type":"code","source":["!ls\n","\n","# Create a blank model or load an existing one\n","#model = MyNetwork(base_net, clf, layers)\n","model = load_model(filename_drive_best)\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"YJLUqftIfTBT","colab_type":"code","colab":{}},"cell_type":"code","source":["train_model(model, epochs, isFinal)#, train_params)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"l2aamGmsg4bU","colab_type":"code","colab":{}},"cell_type":"code","source":["# Tests the model accuracy for each class\n","test_model(model)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"1CGmho82yn1S","colab_type":"code","colab":{}},"cell_type":"code","source":["# Witha tyrained model, plot some training stats to see progress\n","plot_model_stats(model)\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"2daioxenhatj","colab_type":"code","colab":{}},"cell_type":"code","source":[" # This compares several trained model key traiing metrics\n"," #plot_compare_models(model1, model2)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"E8EyzGTMdBwz","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}