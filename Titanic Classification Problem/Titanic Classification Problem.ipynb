{
  "cells": [
    {
      "metadata": {
        "_uuid": "a1c42983471e4dc81f9d6c89c1167b7bafbbfe62",
        "_cell_guid": "3d6e8184-0ae4-4606-a055-0e83eacd72dd"
      },
      "cell_type": "markdown",
      "source": "# Titanic Classification Problem:\n## Machine Learning, Randomized Tuning, Ensemble Voting, Pipelines, and Stacking Models\n## Generative/ Discriminative Models, Nonparametric and Parametric Models, Dimensionality Reduction, Regularization\n\nAuthor: Nick Brooks \n\nDate: November 2017\n\nThe goal of this notebook is to showcase the wide range of models available through the Sklearn wrapper and how to tune them using randomized search. *Note:* In practice, it is usually better to focus one’s energy on a single high performance model, and thoroughly develop its hyperparameters, and of course ensure quality feature selection and engineering. But that is not the goal of this notebook!\n\nSince such a range of models are explored, I also take the opportunity to explain the axis through which these models differ: **Parametric vs Non-Parametric**, and **Generative vs. Discriminative**\n\nAt the end, I also add more flavor to the modeling process by including **Ensemble Voting**, **Pipelining Dimensionality Reduction**, and **Stacking**."
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "b915c98e0541d77934148b4dd83fe03582f02159",
        "_cell_guid": "3660d148-2260-474e-b670-4213ee636754",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# General Packages\nimport pandas as pd\nimport numpy as np\nimport random as rnd\nimport os\nimport re\n# import multiprocessing\n\n# Visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nget_ipython().magic('matplotlib inline')\nplt.rcParams['figure.figsize'] = (16, 8)\nimport scikitplot as skplt\n\n# Supervised Machine Learning Models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, BaggingClassifier, GradientBoostingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron, SGDClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn import feature_selection\nimport xgboost as xgb\nfrom xgboost.sklearn import XGBClassifier # <3\n\n# Unsupervised Models\nfrom sklearn.decomposition import PCA\n\n# Evalaluation\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import confusion_matrix, roc_curve, auc\n\n# Grid\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.feature_selection import RFE\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\nimport scipy.stats as st\n\n# Pipeline\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.pipeline import Pipeline\n\n# Esemble Voting\nfrom mlxtend.classifier import EnsembleVoteClassifier\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report, accuracy_score\n\n# Stacking\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import metrics\nfrom matplotlib.colors import ListedColormap\n\n# Warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport time\nimport datetime\nimport platform\nstart = time.time()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "74199e1c600301e54f1155811a352d2dabd444a5",
        "_cell_guid": "4cd4ace9-9822-480f-b530-91c804eeebc5",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "print('Version      :', platform.python_version())\nprint('Compiler     :', platform.python_compiler())\nprint('Build        :', platform.python_build())\n\nprint(\"\\nCurrent date and time using isoformat:\")\nprint(datetime.datetime.now().isoformat())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "93b5cc513e93ea647f7900cb78c4f0f785aca893",
        "_cell_guid": "b9a1b8be-fc5f-4f0f-a1ac-aaf90510e20f",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Master Parameters:\nn_splits = 5 # Cross Validation Splits\nn_iter = 80 # Randomized Search Iterations\nscoring = 'accuracy' # Model Selection during Cross-Validation\nrstate = 23 # Random State used ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "e5a33a293d6852e99b255166669fdc70bef027c6",
        "_cell_guid": "c1719886-df71-4d9a-b3a6-062d74bfb976"
      },
      "cell_type": "markdown",
      "source": "## Loading and Pre-Processing\n### Cleaning:\n- Category to Numerical representation\n- Dealing with Missing Values, and Filling NA\n\n\n### Feature Selection:\n- Extracting Titles from name variable to use as feature\n- Rescaling continuous variables"
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "8bf9097fa9f39579d8e678c7be607ad472493b7a",
        "_cell_guid": "a6381f5d-02cf-4cc6-84b0-bf97b9ceb837",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Load\ntrain_df = pd.read_csv(\"../input/train.csv\", index_col='PassengerId')\ntest_df = pd.read_csv(\"../input/test.csv\", index_col='PassengerId')\n\n# For Pre-Processing, combine train/test to simultaneously apply transformations\nSurvived = train_df['Survived'].copy()\ntrain_df = train_df.drop('Survived', axis=1)\ndf = pd.concat([test_df, train_df])\ntraindex = train_df.index\ntestdex = test_df.index\ndel train_df\ndel test_df\n\n# New Variables engineering, heavily influenced by:\n# Kaggle Source- https://www.kaggle.com/arthurtok/introduction-to-ensembling-stacking-in-python\n# Family Size\ndf['FamilySize'] = df['SibSp'] + df['Parch'] + 1\n# Name Length\ndf['Name_length'] = df['Name'].apply(len)\n# Is Alone?\ndf['IsAlone'] = 0\ndf.loc[df['FamilySize'] == 1, 'IsAlone'] = 1\n\n# Title: (Source)\n# Kaggle Source- https://www.kaggle.com/ash316/eda-to-prediction-dietanic\ndf['Title']=0\ndf['Title']=df.Name.str.extract('([A-Za-z]+)\\.') #lets extract the Salutations\ndf['Title'].replace(['Mlle','Mme','Ms','Dr','Major','Lady','Countess','Jonkheer','Col',\n                         'Rev','Capt','Sir','Don'],['Miss','Miss','Miss','Mr','Mr','Mrs','Mrs','Other','Other','Other','Mr','Mr','Mr'],inplace=True)\n\ndf.loc[(df.Age.isnull())&(df.Title=='Mr'),'Age']= 33\ndf.loc[(df.Age.isnull())&(df.Title=='Mrs'),'Age']=36\ndf.loc[(df.Age.isnull())&(df.Title=='Master'),'Age']=5\ndf.loc[(df.Age.isnull())&(df.Title=='Miss'),'Age']=22\ndf.loc[(df.Age.isnull())&(df.Title=='Rare'),'Age']=46\ndf = df.drop('Name', axis=1)\n\n# Fill NA\n# Categoricals Variable\ndf['Embarked'] = df['Embarked'].fillna(df['Embarked'].mode().iloc[0])\n# Continuous Variable\ndf['Fare'] = df['Fare'].fillna(df['Fare'].mean())\n\n## Assign Binary to Sex str\ndf['Sex'] = df['Sex'].map( {'female': 1, 'male': 0} ).astype(int)\n# Title\n#df['Title'] = df['Title'].map( {'Mr': 0, 'Mrs': 1, 'Miss': 2, 'Master':3, 'Rare':4} ).astype(int)\n# Embarked\ndf['Embarked'] = df['Embarked'].map( {'Q': 0, 'S': 1, 'C': 2} ).astype(int)\n\n# Get Rid of Ticket and Cabin Variable\ndf= df.drop(['Ticket', 'Cabin'], axis=1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "66b4efed5b19dcbf156ca10f7dfe90ee921fecc4",
        "_cell_guid": "a3cb6998-4399-4d9d-b92f-7478a8175b74"
      },
      "cell_type": "markdown",
      "source": "### Visualization\nBefore Applying dummy variables, I take the opportunity to look at distributions and correlations."
    },
    {
      "metadata": {
        "_uuid": "ba776211956aac877fe480e225434fa27c962d19",
        "_cell_guid": "6631c92a-cf7d-4390-8c27-6c0b87af8e3d",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Histogram\npd.concat([df.loc[testdex, :], Survived], axis=1).hist()\nplt.show()\n\n# Correlation Plot\nsns.heatmap(pd.concat([df.loc[testdex, :], Survived], axis=1).corr(), annot=True, fmt=\".2f\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "5afcc1514fed7cfc60deb4053b930da89423b1fa",
        "_cell_guid": "30e2f4fb-1032-4d8f-824f-40de08efca9c",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Finish Pre-Processing\n# Dummmy Variables\ndf = pd.get_dummies(df, columns=['Embarked','Title','Parch','SibSp','Pclass'], )\n\n# Scaling between -1 and 1. Good practice for continuous variables.\nfrom sklearn import preprocessing\nfor col in ['Fare','Age','Name_length']:\n    transf = df.Fare.reshape(-1,1)\n    scaler = preprocessing.StandardScaler().fit(transf)\n    df[col] = scaler.transform(transf)\n\n# Now that pre-processing is complete, split data into train/test again.\ntrain_df = df.loc[traindex, :]\ntrain_df['Survived'] = Survived\ntest_df = df.loc[testdex, :]\n\n# Dead Weight\ndel df\ndel Survived",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "90bdb94db457b06518863ddd66da625bc6f3a64f",
        "_cell_guid": "942c4be5-e13d-4e65-83bc-24fca8ae79a5"
      },
      "cell_type": "markdown",
      "source": "## Take Your Position!\nDeclaring dependent and independent variables as well as helper functions that record the mean cross-validation average of model accuracy, and its standard deviation to understand the volatility of the models.\n\nAlso includes remnants of my previous \"write submission\" system for those interested."
    },
    {
      "metadata": {
        "_uuid": "fc915ad49fd3eb0bbac70fba33cb7873ce0e3c8c",
        "_cell_guid": "153537fc-771f-4b9d-8364-f74187ef9859",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Depedent and Indepedent Variables\nX = train_df.drop([\"Survived\"] , axis=1)\ny = train_df[\"Survived\"]\nprint(\"X, Y, Test Shape:\",X.shape, y.shape, test_df.shape) # Data Dimensions\n\n# Storage for Model and Results\nresults = pd.DataFrame(columns=['Model','Para','Test_Score','CV Mean','CV STDEV'])\nensemble_models= {}\n\n# Function to \ndef save(model, modelname):\n    global results\n    # Once best model is found, establish more evaluation metrics.\n    model.best_estimator_.fit(X_train, y_train)\n    \n    # submission = model.predict(test_df)\n    # df = pd.DataFrame({'PassengerId':test_df.index,'Survived':submission})\n    # path = ...\n    # df.to_csv((os.path.join(path,(\"submissions/{}.csv\".format(modelname)))),header=True,index=False)\n    \n    scores = cross_val_score(model.best_estimator_, X_train, y_train, cv=5,\n                             scoring=scoring, verbose =0)\n    CV_scores = scores.mean()\n    STDev = scores.std()\n    Test_scores = model.score(X_test, y_test)\n\n\n    # CV and Save scores\n    results = results.append({'Model': modelname,'Para': model.best_params_,'Test_Score': Test_scores,\n                             'CV Mean':CV_scores, 'CV STDEV': STDev}, ignore_index=True)\n    ensemble_models[modelname] = model.best_estimator_\n    \n    # Print Evaluation\n    print(\"\\nEvaluation Method={}\".format(scoring))\n    print(\"Optimal Model Parameters: {}\".format(grid.best_params_))\n    print(\"Train CV Accuracy: %0.2f (+/- %0.2f) [%s]\" % (CV_scores, STDev, modelname))\n    print('Test_Score:', Test_scores)\n        \n    # Scikit Confusion Matrix\n    model.best_estimator_.fit(X_train, y_train)\n    pred = model.predict(X_test)\n    skplt.metrics.plot_confusion_matrix(y_test, pred, title=\"{} Confusion Matrix\".format(modelname),\n                normalize=True,figsize=(6,6),text_fontsize='large')\n    plt.show()\n    # Colors https://matplotlib.org/examples/color/colormaps_reference.html\n\ndef norm_save(model,score, modelname):\n    global results\n    model.fit(X, y)\n    submission = model.predict(test_df)\n    df = pd.DataFrame({'PassengerId':test_df.index, \n                           'Survived':submission})\n    \n    CV_Score = score.mean()\n    Test_scores = model.score(X_test, y_test)\n    STDev = score.std()\n    \n    # CV and Save Scores\n    Test_Score = model.score(X_test, y_test)\n    results = results.append({'Model': modelname,'Para': model,'Test_Score': Test_scores,\n                             'CV Mean': CV_Score, 'CV STDEV': STDev}, ignore_index=True)\n    ensemble_models[modelname] = model\n    \n    print(\"\\nEvaluation Method={}\".format(scoring))\n    print(\"Train CV Accuracy: %0.2f (+/- %0.2f) [%s]\" % (CV_Score, STDev, modelname))  \n    print('Test_Score:', Test_scores)\n        \n    #Scikit Confusion Matrix\n    model.fit(X_train, y_train)\n    pred = model.predict(X_test)\n    skplt.metrics.plot_confusion_matrix(y_test, pred, title=\"{} Confusion Matrix\".format(modelname),\n                normalize=True,figsize=(6,6),text_fontsize='large')\n    plt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "8a500a2bbc2af818cf98ea4167896f9fab8851fc",
        "_cell_guid": "ef69506e-db2f-4772-b153-5d86ce531c25",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# http://scikit-plot.readthedocs.io/en/stable/metrics.html\ndef eval_plot(model):\n    skplt.metrics.plot_roc_curve(y_test, model.predict_proba(X_test))\n    plt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "0a8d1a75f54a3a6558430788eb77c43774a800d6",
        "_cell_guid": "7602ce02-08cb-4e30-ab00-b06d93bf5b3e"
      },
      "cell_type": "markdown",
      "source": "## Imbalanced Dependent Variable\nThis signifies that there is an unequal occurrence of the dependent variable \"Survived\". This could potentially lead to a flawed model. I deal with this by stratifying the train/test groups, leading to equal representation of classes. Additional methods to handle imbalanced datasets include data augmentation and re-sampling methods."
    },
    {
      "metadata": {
        "_uuid": "6fa198cee301888a2a64e69d63b2d21c152cffe1",
        "_cell_guid": "5b1d571b-e678-476e-af49-b6082e44babb",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "print(\"Depedent Variable Distribution\")\nprint(y.value_counts(normalize=True))\n# 0 = Dead\n# 1 = Survived",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "aeb8a7d4d5d57552947a609f1cfe57d3c65b36fd",
        "_cell_guid": "d29e4cd2-40a7-4bd2-9333-748c02d15090"
      },
      "cell_type": "markdown",
      "source": "## Train/Test\nDeclaring dependent and independent variables, as well as preparing the training data into its train-validation-test sets for proper modelling. My validation step is executed during the model fitting process.\n\nNote: Determining model quality through cross-validation is not correct if hyper-parameter tuning is applied, since this leads to a (hyper-parameter) overfitted evaluation of the model. Use an additional untouched test set.\n\nStratified cross validation splits are used. Number of splits is declared at the start of notebook."
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "10b3bbdaced89bf6415fefba9e6ee0afa88d57e0",
        "_cell_guid": "cfd8df9f-6bad-408c-9016-3f57d88fa49a",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Stratified Train/Test Split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y,random_state=rstate)\nX_train.shape, y_train.shape, X_test.shape, y_test.shape\n\n# Stratified Cross-Validation\ncv = StratifiedShuffleSplit(n_splits=n_splits, test_size=0.2, random_state=rstate)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "3f2711de873694436384178ee35751ebea46fbb7",
        "_cell_guid": "7ee56e86-5ed4-4455-adf4-61cedac4a0a6"
      },
      "cell_type": "markdown",
      "source": "# Non-Parametric \nCounterpart to Parametric models, Parametric models does not make any assumptions about the data generating process’ distribution. For example, in statistical test, Non-Parametric models utilize rank and medians, instead of the mean and variance! On the other hand, they function in a infinite space of parameters, making their name counter-intuitive, but also highlighting their practical approach to representation; enabling them to increase their flexibility indefinitely.\n\n# Discriminative Models\nDiscriminative models do not attempt to quantify how the data generating process operates, instead, its goal is to slice and dice the data to classify the data, effectively solving the problem by modeling p(y|x). \n\n## K-Nearest Neighbors\nThe simplest of all machine learning models, a nonparametric method that works well on short and narrow datasets, but seriously struggles in the high dimensional space. It works by using the K nearest point to the predicted point vote on its class (or continuous number when in the regression context). Note: Since this is a instance based learning algorithm, its function is applied locally, and the computation is deferred until the prediction stage. Furthermore, since the data serves as the “map” for new values, the model size may be clunkier than its counterparts. "
    },
    {
      "metadata": {
        "_uuid": "0d856e86197b4f20757ce88d6cb051d21ad6a81f",
        "_cell_guid": "5ff7d417-075a-4042-b0bf-a0ca3c57b011",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Hyper parameters. Since RandomizedSearchCV is used, I use an uniform random interger range for the function to choose from.\nparam_grid ={'n_neighbors': st.randint(1,40),\n            'weights':['uniform','distance']\n            }\n# Hyper-Parameter Tuning with Cross-Validation\ngrid = RandomizedSearchCV(KNeighborsClassifier(),\n                    param_grid, # Hyper Parameters\n                    cv=cv, # Cross-Validation splits. Stratified.\n                    scoring=scoring, # Best-Validation selection metric.\n                    verbose=1, # Quality of Life. Frequency of model updates\n                    n_iter=n_iter, # Number of hyperparameter combinations tried.\n                    random_state=rstate) # Reproducibility \n\n# Execute Tuning on entire dataset\ngrid.fit(X_train, y_train)\n\nsave(grid, \"KNN\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "b9d310e62454fc60a9c42d6222d69bf4de51ac11",
        "_cell_guid": "f8508357-c16f-455f-bdb1-14f5d4ff163a"
      },
      "cell_type": "markdown",
      "source": "## Introduction to Confusion Matrix\n\nThis graphic illustrates the nature of the model mistakes by showing the proportion of false negatives to true negatives, and false positives to true positives. Method is also applicable to results with more than just a binary class.\n\nFalse Negative: When the model labels a positive observation as negative. For example, when the doctor thinks a pregnant women is not pregnant.\nFalse Positive: When a negative observation is predicted to be positive. When a healthy man is (falsely) told he has cancer.\n\nFor the type of sensitive problem described in my examples, model builders may emphasize the minimization of one form of error over the other. We may be more willing to tell healthy person they falsely have cancer, than tell a dying person they have nothing to worry about, and miss the opportunity to receive treatment. \n"
    },
    {
      "metadata": {
        "_uuid": "268efaef1e6c80e128ecd29bf3d9e43b27faf74f",
        "_cell_guid": "af0022c5-eae3-48d2-88b6-575dc4d34053"
      },
      "cell_type": "markdown",
      "source": "### Stochastic Gradient Descent\n\nERROR 404. Adding.. Beep Bop"
    },
    {
      "metadata": {
        "_uuid": "554dae4bf493d16a3869c4777d5e7f055562853b",
        "_cell_guid": "96fc93be-9bc3-48a3-86e1-40179d279f53",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "SGDClassifier().get_params().keys()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "d2fa58a045b66f0c9974488d29e0f4fe7740eba3",
        "_cell_guid": "a7195d34-f955-4c3c-b369-42ff6805e9cf",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "param_grid ={'loss':[\"hinge\",\"log\",\"modified_huber\",\"epsilon_insensitive\",\"squared_epsilon_insensitive\"]\n            }\n\ngrid = GridSearchCV(SGDClassifier(),\n                    param_grid,cv=cv, scoring=scoring,\n                    verbose=1)\n\ngrid.fit(X_train, y_train)\nsave(grid, \"StochasticGradientDescent\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "bf39b27407a9a7291fd09f29f262c973d7a58c95",
        "_cell_guid": "06a9ac4c-2663-46a0-9f4b-5d9d8b4a7723"
      },
      "cell_type": "markdown",
      "source": "# Decision Trees\nI like to think of decision trees as optimizing a series of “If/Then” statements, eventually assigning the value at the tree’s terminal node. Starting from one point at the top, features can then pass the various trials until being assigned its class at the end “leaf” nodes of the tree (technically, it's more like its root tip since the end nodes are visually represented at the bottom of the graph). Trees used here are binary trees, so nodes can only split into two ways."
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "e532bd7c918c8d6ef4ebd471e480c9f7bb62a3c5",
        "_cell_guid": "3bca1ac4-58da-44d2-86bb-95f779fcc864",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Helper Function to visualize feature importance\npredictors = [x for x in train_df.columns if x not in ['Survived']]\ndef feature_imp(model):\n    MO = model.fit(X_train, y_train)\n    feat_imp = pd.Series(MO.feature_importances_, predictors).sort_values(ascending=False)\n    feat_imp.plot(kind='bar', title='Feature Importances')\n    plt.ylabel('Feature Importance Score')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "681edfbf16f8f468f6c91e939cf10c21fb5c7e6c",
        "_cell_guid": "324242fe-fe89-409e-acb1-39a88d039209",
        "trusted": false
      },
      "cell_type": "code",
      "source": "DecisionTreeClassifier().get_params().keys()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "2484054380ad284c8238e03d05f3f00c83048b62",
        "_cell_guid": "09b39255-087f-4cef-8e40-02c4af3220c9",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Baseline Decision Tree\ntree = DecisionTreeClassifier()\nprint(\"Mean CV Accuracy:\",cross_val_score(tree, X, y, cv=cv, scoring=scoring).mean())\nfeature_imp(tree)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "94781c7573ed9c7522732664f3b0f45d3e6f336e",
        "_cell_guid": "e5e759c0-2fd4-4a60-8c8b-c19f4fe622e0"
      },
      "cell_type": "markdown",
      "source": "## Introduction to Feature Importance Graphic\nSince each split in the decision tree distinguishes the dependent variable, splits closer to the root, aka starting point, have optimally been determined to have the greatest splitting effect. The feature importance graphic measures how much splitting impact each feature has. It is important to note that this by no means points to causality, but just like in hierarchical clustering, does point to a nebulous groups. Furthermore, for ensemble tree methods, feature impact is aggregated over all the trees. "
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "9f5620e44fb06aaed39bb2fa15eb686ecd0fde9d",
        "_cell_guid": "6f81cf59-90d9-4a5c-99f2-4fd2396b5892",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# f, ax = plt.subplots(figsize=(9, 6))\n# s = sns.heatmap(pd.crosstab(train_df.Sex, train_df.Title),\n#             annot=True, fmt=\"d\", linewidths=.5, ax=ax,\n#                 cbar_kws={'label': 'Count'})\n# s.set_title('Title Count by Sex Crosstab Heatmap')\n# s.set_xticklabels([\"Mr\",\"Mrs\",\"Miss\",\"Master\",\"Rare\"]);",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "8ab37366c5c52d10a9bb0af8cdece691707e5212",
        "_cell_guid": "7b9dd324-d4ad-4a5e-99fc-3b27afe491bf"
      },
      "cell_type": "markdown",
      "source": "# Ensemble Method for Decision Trees\n\nTechnique where multiple trees are created and then asked to come together and vote on the model’s outcome. Later in this notebook, this same idea is applied to bring together models of different types.\n\nThe sub categories **Boosting** and **Bootstrap Aggregating** are part of this framework, but differ in terms of how the group of trees are *trained*.\n\n## General Hyper-Parameters for Decision Trees and their Ensembles:\n *Sklearn implementation, but universal theory/parameters*\n\nHyperParameters for Tuning:\n- max_features: This is the random subset of features to be considered for splitting operation, the lower the better to reduce variance. For Classification model, ideal max_features = sqr(n_var). \n- max_depth: Maximum depth of the tree. Alternate method of control model depth is *max_leaf_nodes*, which limits the number of terminal nodes, effectively limiting the depth of the tree.\n- n_estimators: Number of trees built before average prediction is made.\n- min_samples_split: Minimum number of samples required for a node to be split. Small minimum would potentially lead to a “Bushy Tree”, prone to overfitting. According to Analytic Vidhya, should be around 0.5~1% of the datasize.\n- min_samples_leaf: Minimum number of samples required at the **Terminal Node** of the tree. In Sklearn, an alternative *min_weight_fraction_leaf* is available to use fraction of the data instead of a fixed integer.\n- random_state: Ensuring consistent random generation, like seed(). Important to consider for comparing models of the same type to ensure a fair comparison. May cause overfitting if random generation is not representative.\n\n### Quality of Life:\nn_jobs: Computer processors utilized. -1 signifies use all processors\nVerbose: Amount of tracking information printed. 0 = None, 1 = By Cross Validation, 1 < by tree.\n\n## Bootstrap aggregating (AKA Bagging) Decision Trees\n\nCreates a bunch of trees using a subset of the the data for each, while using sampling without replacement, which means that values may be sampled multiple times. When combined with cross-validation, the values not sampled, or “held-out”, can then be used as the validation set. This results in a better generalizing model: Less prone to overfitting while maintaining a high model capacity (synonymous with model complexity and flexibility).\n\nThis technique introduces a flavor of the Monte Carlo method, which hopes to achieve better accuracy through sampling.\n\nReading:\nhttps://www.analyticsvidhya.com/blog/2015/06/tuning-random-forest-model/"
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "36f0edcb553081f1578293d5284b1d47b9d366f0",
        "_cell_guid": "4f0ed814-b915-4bba-9cab-d8005581750c",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Parameter Tuning\nparam_grid ={'n_estimators': st.randint(20, 500)}\n\ntree = DecisionTreeClassifier()\ngrid = RandomizedSearchCV(BaggingClassifier(tree),\n                    param_grid, cv=cv, scoring=scoring,\n                    verbose=1,n_iter=n_iter, random_state=rstate)\n\ngrid.fit(X_train, y_train)\nsave(grid, \"Bagger_ensemble\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "135ceb340c6af493063fa16e3f0fc3816d70d15b",
        "_cell_guid": "d6879e13-fd57-446e-9737-5707d0651a7b"
      },
      "cell_type": "markdown",
      "source": "## Random Forest\nBuilds upon Leo Breiman's Bootstrap Aggregation method by adding a random feature selection dimension. \nMore uncorrelated splits, less overemphasis on certain features. Similar to Neural Net’s dropout, since it forces the model to give a large role to less dominant features, leading to a better generalizing model. However, it differs from dropout in the sense that its effect is not repeated and passed over to future iterations of the training process."
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "0ea72fcdbc6ff09a74072383d534d1bb35aef8bb",
        "_cell_guid": "6a532cad-8688-4c50-b441-d40681408e95",
        "scrolled": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "model = RandomForestClassifier()\nprint(\"Mean CV Accuracy:\",cross_val_score(model, X, y, cv=cv, scoring=scoring).mean())\nfeature_imp(model)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "f7219cef558c057cebdc3e7d90d0713b810291ca",
        "_cell_guid": "38a2af84-5dcc-4a07-859b-f9a28e18285d",
        "trusted": false
      },
      "cell_type": "code",
      "source": "RandomForestClassifier().get_params().keys()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "5f135f9df8b025d668acafc4b3b9f381531cbf25",
        "_cell_guid": "15486053-e28c-43ad-a79f-63f4e670ef67",
        "scrolled": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "param_grid ={'max_depth': st.randint(6, 11),\n             'n_estimators':st.randint(300, 500),\n             'max_features':np.arange(0.5,.81, 0.05),\n            'max_leaf_nodes':st.randint(6, 10)}\n\nmodel= RandomForestClassifier()\n\ngrid = RandomizedSearchCV(model,\n                    param_grid, cv=cv,\n                    scoring=scoring,\n                    verbose=1,n_iter=n_iter, random_state=rstate)\n\ngrid.fit(X_train, y_train)\nsave(grid, \"Random_Forest\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "bf2fe26079249abe8ba5bedbcaf39e4f54d745f4",
        "_cell_guid": "7e3f1c45-8343-4ae0-ae7b-9f81324a4737",
        "trusted": false
      },
      "cell_type": "code",
      "source": "feature_imp(grid.best_estimator_)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "46f7fa92464dd8201c9712479a274f397f22f089",
        "_cell_guid": "7a7a5f2c-2c97-4432-ad40-f0635cc4a291"
      },
      "cell_type": "markdown",
      "source": "## Boosting Family\nSimply put, boosting models convert weak models into strong models. Essentially, each weak model is trained on a different distribution of the data, enabling it to learn a narrow rule. When combined, it offers a stronger model. Iteratively, subsequent weak models focus on the source of prediction error from the vote of previous weak models, until the accuracy ceiling is reached.\n\nSource:\nhttps://www.analyticsvidhya.com/blog/2015/11/quick-introduction-boosting-algorithms-machine-learning/\n\n## Adaptive Boosting\n\nApplies weights to all data points and optimizes them using the loss function. Fixes mistakes by assigning high weights to them during iterative process.\n\nIterates through multiple models in order to determine the best boundaries. It relies on using weak models to determine the pattern, and eventually creates a strong combination of them."
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "41beade866b1280ee282682baaf80f37c2d789c2",
        "_cell_guid": "7256a65f-15e8-4a6b-a472-ef1bcf196003",
        "trusted": false
      },
      "cell_type": "code",
      "source": "AdaBoostClassifier().get_params().keys()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "bfbe33cd894ba91d68b31ab627609e88bef67386",
        "_cell_guid": "cb22b9f9-bd5e-4faf-b06b-766a36cd8a52",
        "scrolled": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "param_grid ={'n_estimators':st.randint(50, 400),\n            'learning_rate':np.arange(.1, 4, .5)}\n\ngrid = RandomizedSearchCV(AdaBoostClassifier(),\n                    param_grid,cv=cv, scoring=scoring,\n                    verbose=1, n_iter=n_iter, random_state=rstate)\n\ngrid.fit(X_train, y_train)\nsave(grid, \"AdaBoost_Ensemble\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "a56f8403d2e8caa3ec164babf67aa88107d3f2a4",
        "_cell_guid": "f716f970-8f08-4b76-a124-609e4e6e75a0",
        "trusted": false
      },
      "cell_type": "code",
      "source": "feature_imp(grid.best_estimator_)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "b1e4e081388184c0f1a369d3eaac87ed50c76a0b",
        "_cell_guid": "6e46e720-04c8-4d96-9310-76dff2a003d7"
      },
      "cell_type": "markdown",
      "source": "\n## Gradient Boosting Classifier\n\n### Additional Gradient Boosting Hyper-Parameters:\nLearning_rate: How much parameters are updated after each iteration of gradient descent. Low mean smaller steps, most likely to reach the global minimum, although there are cases where this doesn’t always work as intended.\nN_estimators: Note this is still the number of trees being built, but it within the GBC’s sequential methodology.\nSubsample: Similar to the Bootstrap Aggregate method, controlling percentage of data utilized for a tree, although the standard is to sample *without* replacement. In this model, operates in similar ways to Stochastic Gradient Descent in Neural Networks, but with large batch sizes. Note this decision trees are still a *Shallow Model*, so it is still profoundly different from Neural Networks.\nLoss: Function minimized by gradient descent.\nInit: Initialization of the model internal parameters. May be used to build off another model's’ outcome.\n\nAs the name suggest, Gradient Boosting iteratively trains models sequentially to minimize Loss, a convex optimization method similar to that seen in Neural Networks.\n\nhttps://www.ncbi.nlm.nih.gov/pmc/articles/PMC3885826/\n\nThis is a Greedy Algorithm, which makes the most favorable split when it can. This means it's short sighted and will also stop when the loss doesn’t improve.\n\nMore information:\n\nhttps://www.analyticsvidhya.com/blog/2016/02/complete-guide-parameter-tuning-gradient-boosting-gbm-python/\n\nhttps://www.ncbi.nlm.nih.gov/pmc/articles/PMC3885826/"
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "85161b7e3a7295e187449a4ee5a139c8c61aeb8d",
        "_cell_guid": "f18b3b44-57eb-48dd-82ec-8252c5573286",
        "trusted": false
      },
      "cell_type": "code",
      "source": "GradientBoostingClassifier().get_params().keys()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "79e4e4f16a4b7da5755a1e446a847f3932bc078f",
        "_cell_guid": "1b2a6d6a-8f11-4020-bad0-88be8e0c4c77",
        "scrolled": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "param_grid ={'n_estimators':st.randint(100, 400),\n            'loss': ['deviance', 'exponential'],\n            'learning_rate':np.arange(0.01, 0.32,.05),\n            'max_depth': np.arange(2, 4.1, .5)}\n\ngrid = RandomizedSearchCV(GradientBoostingClassifier(),\n                    param_grid,cv=cv,\n                    scoring=scoring,\n                    verbose=1, n_iter=n_iter, random_state=rstate)\n\ngrid.fit(X_train, y_train)\nsave(grid, \"Gradient_Boosting\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "8a2b29e2d432f20b8a524727f1198744ad1848f6",
        "_cell_guid": "8a7977dd-d71a-4e82-b170-e9367be55a8b",
        "trusted": false
      },
      "cell_type": "code",
      "source": "feature_imp(grid.best_estimator_)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "f286a724b2ca653f7acddb2ab115dd99ec29974a",
        "_cell_guid": "fc8c0613-3e83-46f0-b9d7-2ff59b438619"
      },
      "cell_type": "markdown",
      "source": "## XGBoost - eXtreme Gradient Boosting\nAs the name suggest, Gradient Boosting on steroids. Infact, multiple steroids:\nRegularization\nComputation Speed and Parallel Processing\nHandles Missing Values\nImproves on Gradient Boosting ‘*Greedy*’ tendencies. It does this by considering reaching the max depth and retroactively pruning \nInterruptible and Resumable, as well as Hadoop capabilities.\n\nXGBoost is able to approximate the Loss function more efficiently, thereby leading to faster computation and parallel processing. Inside its objective function, it has also incorporated a regularization term, ridge or lasso, to stay clear of unnecessary high dimensional spaces and complexity.\n\n### Hyper-Parameters for Tree Classification Booster (Sklearn Wrapper)\nMin_child_weight: Similar strand to *min_child_leaf*, which controls the minimum number of observation to split to a terminal node. This instead relates to defines the minimum sum of derivatives found in the Hessian (second-order)of all observations required in a child.\nGamma: Node is split only if it gives a positive decrease in the loss function. Higher performance regulator of complexity.\nMax_delta_step: What the tree’s weights can be.\nColsample_bylevel: Random Forest characteristic, where it enables subfraction of features to be randomly selected for each tree.\n\n### Regularization\nIn the context of GBMs, regularization through shrinkage is available, but applying it to the model with the tree base-learner is different from its traditional coefficient constriction. For GBM Trees, the shrinking decreases the influence of each additional tree in the iterative process, effectively applying a decay of impact over boosting rounds. As a result, it no longer searches as the feature selection method, since it cannot adjust the coefficients of each feature (and potential interaction/ polynomial terms).\n\n\n### Learning Task Parameters:\nObjective: binary:logistic = Outputs probability for two classes. Multi:softmax = outputs prediction for *num_class* classes. Multi:prob = probability for 3 or more classes.\nThe objective is merely a matter of catering to the data type, although additional protocols on the probabilities may be set up. For example, the highest probability below a certain threshold may be deemed an inadequate score, passed over for human processing. The optimal loss function should cater to the behavior of the data and goal at hand, and usually requires input from the domain knowledge base.\n\nThe Evaluation Metric (*eval_metric*) has important implications depending on the problem at hand. If the dataset is imbalanced and the minority class is of interest, it is better to use [AUC] Area Under the Curve than accuracy or rmse, since those metrics can get away with assigning the majority class to all, and still get away with high accuracy!\n\n\nMore Info:\nhttps://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/\n\n\nPython Installation: https://www.ibm.com/developerworks/community/blogs/jfp/entry/Installing_XGBoost_For_Anaconda_on_Windows?lang=en"
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "497861b785c6f5d1010664b3ae05834eaab3a054",
        "_cell_guid": "f6ba1ec6-4ae6-4f5d-9288-e11545ad480e",
        "trusted": false
      },
      "cell_type": "code",
      "source": "XGBClassifier().get_params().keys()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "efd3b00fe4ac7a3d19512e6178fca00197f20e7f",
        "_cell_guid": "f1bbc24d-5ae3-43b7-bceb-bf9bdcc50c79",
        "trusted": false
      },
      "cell_type": "code",
      "source": "one_to_left = st.beta(10, 1)  \nfrom_zero_positive = st.expon(0, 50)\n\nparams = {  \n    \"n_estimators\": st.randint(100, 500),\n    \"max_depth\": st.randint(2, 8),\n    \"learning_rate\": [0.01],\n    #'max_features':'sqrt',\n    #\"learning_rate\": st.uniform(0.001, 0.1),\n    \"colsample_bytree\": one_to_left,\n    \"subsample\": one_to_left,\n    \"gamma\": st.uniform(0, 3),\n    #'reg_alpha': from_zero_positive,\n    #\"min_child_weight\": from_zero_positive,\n    'reg_alpha':[0, 0.001, 0.005, 0.01, 0.05]\n}\n\nxgbreg = XGBClassifier(objective= 'binary:logistic', eval_metric=\"auc\",\n                       nthreads=2)\n\ngrid = RandomizedSearchCV(xgbreg, params, n_jobs=1, verbose=1, n_iter=n_iter,\n                          random_state=rstate, scoring=scoring)  \ngrid.fit(X_train,y_train, verbose=False)\nsave(grid, \"Sci_kit XGB\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "2cbf72b7c3fa92fad77e3103cb3c581d15f6379c",
        "_cell_guid": "876d9000-17c1-4186-827b-7c9566a207a0",
        "trusted": false
      },
      "cell_type": "code",
      "source": "feature_imp(grid.best_estimator_)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "525d49c276b280bc80ebb29d32c6e936566fe935",
        "_cell_guid": "0f50322a-f90e-4a55-949d-eb8041a7b9e6",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# What is going on with Age, Sex and Survival?\nwith sns.axes_style(style='ticks'):\n    g = sns.factorplot(\"Sex\", \"Age\", \"Survived\", data=train_df, kind=\"box\")\n    g.set(ylim=(-1,5))\n    g.set_axis_labels(\"Sex\", \"Age\");",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "4914faca99ac7aefd9de3f89de820da2950266f6",
        "_cell_guid": "0437109d-e0b2-4944-9dc6-b716c630abcb",
        "trusted": false
      },
      "cell_type": "code",
      "source": "num_rounds = 1000\nmodel = XGBClassifier(n_estimators = num_rounds,\n                        objective= 'binary:logistic',\n                     learning_rate=0.01, random_state=rstate, scoring=scoring)\n\n# use early_stopping_rounds to stop the cv when there is no score imporovement\nmodel.fit(X_train,y_train, early_stopping_rounds=20, eval_set=[(X_test,\ny_test)], verbose=False)\nscore = cross_val_score(model, X_train,y_train, cv=cv)\nprint(model)\nprint(\"\\nxgBoost - CV Train : %.2f\" % score.mean())\nprint(\"xgBoost - Train : %.2f\" % metrics.accuracy_score(model.predict(X_train), y_train))\nprint(\"xgBoost - Test : %.2f\" % metrics.accuracy_score(model.predict(X_test), y_test))\nnorm_save(model,score, \"XGBsklearn\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "fac125a53f01d14a814f54681958c8f5f570a6b4",
        "_cell_guid": "fe0fbaf2-840a-40e8-987b-8302ccc5a6f2"
      },
      "cell_type": "markdown",
      "source": "## XGBoost Package Implementation"
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "9b995ec401f605871d31a50f31eb6c83d823f484",
        "_cell_guid": "ab178e5a-9019-4d7b-9749-51267e036cb0",
        "scrolled": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "num_rounds = 100\nxgtrain = xgb.DMatrix(X_train, label=y_train)\nxgtest = xgb.DMatrix(X_test, label=y_test)\n\n# set xgboost params\nparam = {'max_depth': 3,  # the maximum depth of each tree\n         'objective': 'binary:logistic'}\n\nclf_xgb_cv = xgb.cv(param, xgtrain, num_rounds, \n                    stratified=True, \n                    nfold=n_splits, \n                    early_stopping_rounds=20)\nprint(\"Optimal number of trees/estimators is %i\" % clf_xgb_cv.shape[0])\n\nwatchlist  = [(xgtest,'test'), (xgtrain,'train')]                \nclf_xgb = xgb.train(param, xgtrain,clf_xgb_cv.shape[0], watchlist)\n\n# predict function will produce the probability \n# so we'll use 0.5 cutoff to convert probability to class label\ny_train_pred = (clf_xgb.predict(xgtrain, ntree_limit=clf_xgb.best_iteration) > 0.5).astype(int)\ny_test_pred = (clf_xgb.predict(xgtest, ntree_limit=clf_xgb.best_iteration) > 0.5).astype(int)\nscore= metrics.accuracy_score(y_test_pred, y_test)\nprint(\"\\nXGB - Train : %.2f\" % score)\nprint(\"XGB - Test : %.2f\" % metrics.accuracy_score(y_train_pred, y_train))\nnorm_save(model,score, \"XGBstandard\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "a7ef25067cae000c13edd6f73b57135510056367",
        "_cell_guid": "a21b80c0-0b54-4d94-a03e-fa9fd98ea656"
      },
      "cell_type": "markdown",
      "source": "# Parametric Models\nFamily of models which makes assumption of the underlying distribution, and attempts to build models on top of it with a fixed number of parameters. Works great when they're assumption is correct and the data behaves itself.\n\n# Parametric Discriminative Models\nI want to take this opportunity to elaborate a bit more on the discriminative/generative dichotomy. An interesting observation by Andrew Ng is that while discriminative classifiers can reach a higher accuracy cap (asymptotic error), it achieves it at a slower rate than its generative counterpart. He does this by comparing *Naive Bayes* and *Logistic Regression*, two generative models. Therefore, for computational and goal solving reasons, it is best to solve for the conditional probability of p(x|y) directly, instead of trying to compute their joint distribution as seen in generative models.\n \nSources:\nhttps://ai.stanford.edu/~ang/papers/nips01-discriminativegenerative.pdf\n\n# Logistic Regression\nThe classification regression. In the binary classification problem, the classic linear regression is unable to bound itself between classes [0,1]. So the logistic regression uses the logit from the sigmoid function, which transformed the weighted input into a probability of class being “1”. Although not the best performer, if offers exploratory information and can potentially reveal causal information, if the experiment is setup correctly."
    },
    {
      "metadata": {
        "_uuid": "088f5cf20adf3b4de50bfe715f68b9df472e87db",
        "_cell_guid": "35c77da3-7b88-4b94-b892-4cf1adb895cf",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "model= LogisticRegression()\nscore = cross_val_score(model, X_train, y_train, cv=cv, scoring='accuracy')\nnorm_save(LogisticRegression(),score, \"Logistic_Regression\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "45d95021e1009e39beb2acad4dfebc38f5e7ca44",
        "_cell_guid": "426832b3-4278-4d62-a2ff-bf5666581a09",
        "scrolled": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "# import statsmodels.api as sm\n# logit = sm.Logit(y, X) # fit the model\n# #logit.fit().summary()\n# #result.summary()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "99df0deb349ba7801853894be2f94d41734eb71b",
        "_cell_guid": "362906dc-a698-4feb-97da-cb2d7ffe8e8b"
      },
      "cell_type": "markdown",
      "source": "# Neural Networks\nThe only “Deep Model” out of the mix. This is a characteristic of *Representation Learning*, which effectively grants the model its own feature processing and selection steps, catering to large, complex data with intertwining effects. In computer vision tasks, the convolutional neural networks is able to piece apart corners, colors, patterns and more. Recent research in Style Transfer even suggests that stylistic properties of a picture, such as art, can be extracted, and controlled.  (Source: https://arxiv.org/abs/1611.07865)\n\nA common explanation for Neural Networks is that it is a whole bunch of Logistic Regressions. An important thing to remembers is that the hidden-layers are fully connected, meaning that each input variables has a weight to each node (hidden-unit) in the hidden layer, thereby resulting in a black box with a whole lot of parameters, and a whole lot of matrix multiplications. Finally it's, ironic that one of the most intelligible classifiers can be transformed into the least intelligible! In *Computer Age Statistical Inference* authors Bradley Efron and Trevor Hastie are hopeful for the next Ronald Fisher to come and provide statistical intelligibility to modern day machine learning models, many of which are highly developed computationally, but lacking inferential theory.\n\nModel not ideal for such as small dataset."
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "739e00880b371503483465aae311693d77169d3c",
        "_cell_guid": "05430881-6959-4114-bb3a-6ac30e6eb33e",
        "trusted": false
      },
      "cell_type": "code",
      "source": "MLPClassifier().get_params().keys()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "dde67af506453990feff1fb2972f4e7f3c17f8e3",
        "_cell_guid": "8921af97-ff6b-48fb-b4bd-2082f2f08fc1",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Start with a RandomSearchCV to efficiently Narrow the Ballpark\nparam_grid ={'max_iter': np.logspace(1, 5, 10).astype(\"int32\"),\n             'hidden_layer_sizes': np.logspace(2, 3, 4).astype(\"int32\"),\n             'activation':['identity', 'logistic', 'tanh', 'relu'],\n             'learning_rate': ['adaptive'],\n             'early_stopping': [True],\n             'alpha': np.logspace(2, 3, 4).astype(\"int32\")\n            }\n\nmodel = MLPClassifier()\n\ngrid = RandomizedSearchCV(model,\n                    param_grid, cv=cv, scoring=scoring,\n                    verbose=1, n_iter=n_iter, random_state=rstate)\n\ngrid.fit(X_train, y_train)\nsave(grid, \"FFNeural_Net\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "5ff27c4effda35c2c790dbab0adb68ffeaa3cd57",
        "_cell_guid": "9dcc0f62-dc4d-48e5-8b71-c231e9ec7ac4"
      },
      "cell_type": "markdown",
      "source": "# Support Vector Classifier\nAnother model originating in Computer Science, the Support Vector Classifier creates a separation between point to determine class. Maximizing the accuracy of the discriminatory protocol.\n\n### Hyperparameters:\n- C: Rigidity and size of the separation line margin. Increasing C leads to a smaller, thus a more complex mode, and everything that comes with it (High variance, Low Bias)!\n\n## Linear Classifier:\nIn this case, the separator is linear. Imagine a two dimensional plot with a straight line separating two classes of points, but efficiently scalable up to a high dimensional space (plane, cube… hypercube :-O ? )"
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "1a289723ab119c26b64f0cfc280cc83d65725435",
        "_cell_guid": "63c49795-7f11-4115-b92b-818c481d2ddf",
        "trusted": false
      },
      "cell_type": "code",
      "source": "LinearSVC().get_params().keys()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "ece511661ec599e4725543f9790addbb358ed552",
        "_cell_guid": "6accbf41-ce25-480d-93a8-bf2d7387babb",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Define Model\nmodel = LinearSVC()\n\n#Fit Model\nscores= cross_val_score(model, X_train, y_train, cv=cv, scoring=scoring)\nnorm_save(model, scores, \"LinearSV\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "222c33ada84ae478322c1220df257e22733c89c4",
        "_cell_guid": "d5d0f824-43e6-4ddc-b23b-a02b31723ee2"
      },
      "cell_type": "markdown",
      "source": "## Radial Basis Function (RBF)\nHere, not only are non-linear boundaries available, the kernel trick is aso introduced, which enables new representations of the data to be formulated, effectively granting the models new dimensions to find better separators in.\n\n### Hyperparameter:\n- Gamma: How far the influence of a single training example reaches, and sort of like a soft boundary with a gradient. \n- Low Gamma -> Distant fit, High Gamma = Close Fit, Inverse of the radius of influence of samples selected by the model as support vectors.\nhttp://pages.cs.wisc.edu/~yliang/cs760/howtoSVM.pdf"
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "41eb2e2308f233e9198d7ec22fcf7ea789ef2c64",
        "_cell_guid": "ac9b46de-5fa6-46f1-8db3-d5bfdcc6a8ff",
        "trusted": false
      },
      "cell_type": "code",
      "source": "SVC().get_params().keys()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "89842b50648f93d1e82aebb1e10220bda1f1c164",
        "_cell_guid": "d76881ff-eac8-410f-b8c1-ae5c9361377a",
        "trusted": false
      },
      "cell_type": "code",
      "source": "svc = SVC(kernel= 'rbf', probability=True)\n\nmodel = Pipeline(steps=[('svc', svc)])\n\n\nparam_grid = {'svc__C': st.randint(1,10000),\n              'svc__gamma': np.logspace(1, -7, 10)}\n\ngrid = RandomizedSearchCV(model, param_grid,\n                          cv=cv, verbose=1, scoring=scoring,\n                         n_iter=n_iter, random_state=rstate)\n\ngrid.fit(X_train, y_train)\nsave(grid, \"SVCrbf\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "aaec2ccf07c587981e84d97ae71934b6594a059b",
        "_cell_guid": "66d50251-5db1-4f08-b3fc-c8fb0c2d7f69"
      },
      "cell_type": "markdown",
      "source": "## Pipeline: Principle Components Analysis and Support Vector Classifier\nPipelines enable multiple models or processes to be chained up and hyper-parameter tuned together. Very powerful tool, especially when uncertain about a model's reaction to processed data, since it may reveal complex, high performing combinations.\n\n### Dimensionality Reduction: Principal Component Analysis\nDecreases the noise by condensing the features into the dimensions of most variance. "
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "7e870663ebef41a0ffecaae16d5de303856ccd39",
        "_cell_guid": "19415cfa-115f-4006-b368-74d65c32419f",
        "trusted": false
      },
      "cell_type": "code",
      "source": "pca = PCA()\nsvc = SVC(kernel= 'rbf',probability=True)\n\nmodel = Pipeline(steps=[('pca',pca),\n                        ('svc', svc)])\n\n\nparam_grid = {'svc__C': st.randint(1,10000),\n              'svc__gamma': np.logspace(1, -7, 10),\n             'pca__n_components': st.randint(1,len(X.columns))}\n\ngrid = RandomizedSearchCV(model, param_grid,\n                          cv=cv, verbose=1,\n                         n_iter=n_iter, random_state=rstate, scoring=scoring)\n\ngrid.fit(X_train, y_train)\nsave(grid, \"PCA_SVC\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "7352299fac6e5b2b4f29b7be7536fedd9dd4707f",
        "_cell_guid": "e7870285-8678-4754-be2a-423579c492aa"
      },
      "cell_type": "markdown",
      "source": "# Parametric Generative Classification\nModels the data generating process in order to assign which categorize the features. Process is represented in terms of joint probabilities. Advantageous since it can be used to generate features, but tends to struggle in terms of accuracy and doesn’t scale well into tall and wide datasets. Only Gaussian Bayes used in this category, but additional Generative Models can be explored, such as:\n\nHidden Markov model\nProbabilistic context-free grammar\nAveraged one-dependence estimators\nLatent Dirichlet allocation\nRestricted Boltzmann machine\nGenerative adversarial networks\n\n## Gaussian Naive Bayes\nThrough Bayes rule, is able to use conditional probabilities to classify data. Predicts the class by finding the one with the highest Posterior.\n\n### Interpretation:\nAfter I increased the feature space through feature engineering, model performance dropped from 75% to worst than random. Furthermore, this model gambles that most passengers died. Only thing going for it is its low false negative rate.."
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "1dd8609afa670fd4d0582f384f77f7480774b789",
        "_cell_guid": "fbf430de-089d-4db7-b07c-d3471c31f8ce",
        "trusted": false
      },
      "cell_type": "code",
      "source": "model = GaussianNB()\n\nscore = cross_val_score(model, X_train, y_train, cv=cv, scoring=scoring)\nnorm_save(model,score, \"Gaussian\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "8bed3cf19c0489fd4cef74f59462a1c0066ac61e",
        "_cell_guid": "77d9979e-a943-4362-b5cb-3e7eda9867d5"
      },
      "cell_type": "markdown",
      "source": "## Results"
    },
    {
      "metadata": {
        "_uuid": "4076101595aa10d7f3cecfe1ba0d73de4c8bc187",
        "_cell_guid": "17e0c5a3-fdbd-43db-883f-9120f252a72f",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "results = results.sort_values(by=[\"CV Mean\"], ascending=False)\nresults",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "a7b154f256870d4488d18353fb3f20bc21d996e5",
        "_cell_guid": "40b60417-ff5a-4cab-97b6-567f0d6275cf"
      },
      "cell_type": "markdown",
      "source": "## Classification Voting: Model Ensemble\nLike the system that enabled multiple trees to predict together, this system brings together models of all types. A broader implementation.\n\n- **Hard Voting:** Plurality voting over the classes.\n- **Soft Voting:** Selects classed based off aggregated probabilities over the models. Requires model with probabilistic capabilities, which is why I removed linear SCV from inclusion. This difference is tremendous, while hard votes may polarize a small difference between models, say the prediction of 51% Survived and 49% Dead, the soft voting is able to make a more nuanced decision, rewarding high confidence models, and penalizing low confidence.\n- **Weights:** This is an additional feature which enables manual assigning voting power to each voting model.\n\n## Prepare and Observe Voting Models"
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "79af06870e10db6491e6ba8185e193a43a757d43",
        "_cell_guid": "ad53e70a-24e6-4e47-afa2-21fadf41efbe",
        "trusted": false
      },
      "cell_type": "code",
      "source": "### Ensemble Voting\nnot_proba_list = ('LinearSV')\nnot_proba =  results[results.Model == not_proba_list]\nhard_models = results # All Can Be\nprob_models = results[results.Model != not_proba_list] # Not All",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "1697c52a9fe26800006848447fc626203ddd8527",
        "_cell_guid": "901db939-5ac1-4e9b-9e39-2c8c7c4b3d14",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# None Probabilistic\nmodels = list(zip([ensemble_models[x] for x in not_proba.Model],\n                  not_proba.Model))\nclfs = []\nprint('5-fold cross validation:\\n')\nfor clf, label in models:\n    scores = cross_val_score(clf, X_train, y_train, cv=cv, scoring=scoring, verbose=0)\n    print(\"Train CV Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))\n    md = clf.fit(X_train, y_train)    \n    clfs.append(md)\n    print(\"Test Accuracy: %0.2f \\n\" % (metrics.accuracy_score(clf.predict(X_test), y_test)))\n    submission = md.predict(test_df)\n    df = pd.DataFrame({'PassengerId':test_df.index, \n                           'Survived':submission})\n    df.to_csv(\"{}.csv\".format(label),header=True,index=False)\n\ndel clfs",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "6507e403bc5ac165b2799b727a19fa5d94c04818",
        "_cell_guid": "bf8f3b1f-81ba-4273-8191-ccaf24eaab6f",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Only Probabilistic\nmodels = list(zip([ensemble_models[x] for x in prob_models.Model],\n                  prob_models.Model))\nplt.figure()\n\nprint('5-fold cross validation:\\n')\nclfs = []\n#[ensemble_models[x] for x in prob_models.Model]\nfor clf, label in models:\n    scores = cross_val_score(clf, X_train, y_train,cv=5, scoring=scoring, verbose=0)\n    print(\"Train CV Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))\n    md = clf.fit(X_train, y_train)\n    # Add to Roc Curve\n    fpr, tpr, _ = roc_curve(y_test, md.predict_proba(X_test)[:,1])\n    roc_auc = auc(fpr, tpr)\n\n    print('ROC AUC: %0.2f' % roc_auc)\n    plt.plot(fpr, tpr, label='{} ROC curve (area = {:.2})'.format(label, roc_auc))\n    \n    clfs.append(md)\n    print(\"Test Accuracy: %0.2f \\n\" % (metrics.accuracy_score(clf.predict(X_test), y_test)))\n    \n    submission = md.predict(test_df)\n    df = pd.DataFrame({'PassengerId':test_df.index, \n                           'Survived':submission})\n    df.to_csv(\"{}.csv\".format(label),header=True,index=False)\n\n# Plot\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.legend(loc=\"lower right\")\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "3816561b05d1395425940f9549bf06a59801106d",
        "_cell_guid": "07575886-fc32-4e75-b759-0afcfb733c63"
      },
      "cell_type": "markdown",
      "source": "Like seen in the individual confusion matrixs, the ROC curve demonstrates that most of the models have a similar tradeoff between false positive and false negatives. \n\n\n## Soft and Hard Voter of Difference Sizes"
    },
    {
      "metadata": {
        "_uuid": "f19dd8a225334d70711e6b89a8a26a664943d63f",
        "_cell_guid": "f8d5c5eb-deb9-4321-9476-81c3fef55df3",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Play with Weights\nplt.figure()\nfor x in [2,3,5,7,10]:\n    ECH = EnsembleVoteClassifier([ensemble_models.get(key) for key in hard_models.Model[:x]], voting='hard')\n    ECS = EnsembleVoteClassifier([ensemble_models.get(key) for key in prob_models.Model[:x]], voting='soft')\n    print('\\n')\n    print('{}-Voting Models: 5-fold cross validation:\\n'.format(x))\n    \n    for clf, label in zip([ECS, ECH], \n                          ['{}-VM-Ensemble Soft Voting'.format(x),\n                           '{}-VM-Ensemble Hard Voting'.format(x)]):\n        scores = cross_val_score(clf, X_train, y_train, cv=5, scoring='accuracy')\n        print(\"Train CV Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))\n        md = clf.fit(X_train, y_train)    \n        clfs.append(md)\n\n        if clf is ECS:\n             # Add to Roc Curve\n            fpr, tpr, _ = roc_curve(y_test, md.predict_proba(X_test)[:,1])\n            roc_auc = auc(fpr, tpr)\n            print('ROC AUC: %0.2f' % roc_auc)\n            plt.plot(fpr, tpr, label='{} ROC curve (area = {:.2})'.format(label, roc_auc))\n        \n        \n        Test_Score = metrics.accuracy_score(clf.predict(X_test), y_test)\n        print(\"Test Accuracy: %0.2f \" % Test_Score)\n        \n        CV_Score = scores.mean()\n        STDev = scores.std()\n        \n        global results\n        results = results.append({'Model': label,'Para': clf, 'CV Mean': CV_Score,\n                'Test_Score':Test_Score,'CV STDEV': STDev}, ignore_index=True)\n        ensemble_models[label] = model\n        \n        submission = md.predict(test_df)        \n        df = pd.DataFrame({'PassengerId':test_df.index,'Survived':submission})\n        df.to_csv(\"{}.csv\".format(label),header=True,index=False)\n        \n# Plot\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Soft Model ROC Curve')\nplt.legend(loc=\"lower right\")\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "7ae04a3dc34df1e9cd65c7d7e05b8fb05a8ce3c8",
        "_cell_guid": "ad50f400-a389-4608-961c-0ba974cb4ea3"
      },
      "cell_type": "markdown",
      "source": "## Stacking Models\n“Stacking is a way of combining multiple models, that introduces the concept of a meta learner. It is less widely used than bagging and boosting. Unlike bagging and boosting, stacking may be (and normally is) used to combine models of different types.” - Anshul Joshi [1]\n\nBig shoutout to Manohar Swamynathan, author of Mastering Machine Learning with Python in Six Steps, who eloquently explains and took me step by step through stacks in Python. [2]\n\nNext:\nCould target high false negative through stacking methods, since it builds models off model weakness.\n\nRead more here:\n\n[1] https://www.quora.com/What-is-stacking-in-machine-learning \n\n[2] https://github.com/Apress/mastering-ml-w-python-in-six-steps/blob/master/Chapter_4_Code/Code/Stacking.ipynb"
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "4ceccc26582407d2c7784e98819931064a0d1637",
        "_cell_guid": "73240db0-074a-413d-9084-702133bd358b",
        "trusted": false
      },
      "cell_type": "code",
      "source": "from sklearn import cross_validation\n\nX = train_df.drop([\"Survived\"] , axis=1)\ny = train_df[\"Survived\"]\n\n#test_df  = test_df.drop([\"PassengerId\"] , axis=1).copy()\nprint(X.shape, y.shape, test_df.shape)\n\n#Normalize\n# X = StandardScaler().fit_transform(X)\n\n# ensemble_models.values()\n\n# evaluate the model by splitting into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=rstate)\nkfold = cross_validation.StratifiedKFold(y=y_train, n_folds=5, random_state=rstate)\nnum_trees = 10\nverbose = True # to print the progress\n\nclfs = [KNeighborsClassifier(),\n        RandomForestClassifier(n_estimators=num_trees, random_state=rstate),\n        GradientBoostingClassifier(n_estimators=num_trees, random_state=rstate)]\n\n# Creating train and test sets for blending\ndataset_blend_train = np.zeros((X_train.shape[0], len(clfs)))\ndataset_blend_test = np.zeros((X_test.shape[0], len(clfs)))\ndataset_blend_test_df = np.zeros((test_df.shape[0], len(clfs)))\n\nprint('5-fold cross validation:')\nfor i, clf in enumerate(clfs):   \n    scores = cross_validation.cross_val_score(clf, X_train, y_train, cv=kfold, scoring='accuracy')\n    print(\"##### Base Model %0.0f #####\" % i)\n    print(\"Train CV Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std()))\n    clf.fit(X_train, y_train)   \n    print(\"Train Accuracy: %0.2f \" % (metrics.accuracy_score(clf.predict(X_train), y_train)))\n    dataset_blend_train[:,i] = clf.predict_proba(X_train)[:, 1]\n    dataset_blend_test[:,i] = clf.predict_proba(X_test)[:, 1]\n    dataset_blend_test_df[:,i] = clf.predict_proba(test_df)[:, 1]\n    print(\"Test Accuracy: %0.2f \\n\" % (metrics.accuracy_score(clf.predict(X_test), y_test)))    \n\nprint(\"##### Meta Model #####\")\nclf = LogisticRegression()\nscores = cross_validation.cross_val_score(clf, dataset_blend_train, y_train, cv=kfold, scoring=scoring)\nclf.fit(dataset_blend_train, y_train)\nprint(\"Train CV Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std()))\nprint(\"Train Accuracy: %0.2f \" % (metrics.accuracy_score(clf.predict(dataset_blend_train), y_train)))\nprint(\"Test Accuracy: %0.2f \" % (metrics.accuracy_score(clf.predict(dataset_blend_test), y_test)))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "deeb56a5d5802689b1e1bf91f086c028f3a49d6c",
        "_cell_guid": "3df34e06-ec5e-4b2d-88e1-c0196a5bdce6",
        "trusted": false
      },
      "cell_type": "code",
      "source": "#eval_plot(clf)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "ad4bdfb37a740a67af5e2c50bf1217cf40a8c7c9",
        "_cell_guid": "1afced3f-78d6-426e-93b5-29e99e1c5531",
        "trusted": false
      },
      "cell_type": "code",
      "source": "score = cross_val_score(clf, X, y, cv=cv, scoring=scoring)\nnorm_save(clf, score, \"stacked\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "1dbb78bd52a7de90f28e5d56c31e48d9b227f09e",
        "_cell_guid": "1cf73039-3f8b-41dd-a3c6-4b94cea44127"
      },
      "cell_type": "markdown",
      "source": "## Best Model: Soft Voting Ensemble with Top Seven Models\n\nAfter shoveling most of these models into Kaggle, I found that the **Soft Voting Ensemble with Top Seven Models** to be the best performing with a public score of **81.339** (not reproducible yet.. oops). I am curious to see whether model deviation, or CV Train and Test score difference are able to indicate submission set performance, since it may shed light on model consistency."
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "34fffdb6551631b2966bef4f0269e5669f8d8e55",
        "_cell_guid": "861b905a-bd98-42ac-9cda-26a6675e1eb6",
        "trusted": false
      },
      "cell_type": "code",
      "source": "results.sort_values(by=[\"CV Mean\"], ascending=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "8a25288117de7481832a23b556ab579d8a2ad066",
        "_cell_guid": "468b9b29-c024-4591-acc1-baefdaec50c6"
      },
      "cell_type": "markdown",
      "source": "## Classification Evaluation for Best Model"
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "33c83d6cd68553ca0eac4061138a8941f1bcf5ba",
        "_cell_guid": "0ac80cdb-198c-4fb1-b25f-39f45ff09d19",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# http://scikit-plot.readthedocs.io/en/stable/Quickstart.html",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "ecf99ef35056ee79dd268d7134502cf9fa486b36",
        "_cell_guid": "99a59f8b-64ce-4a46-8952-acfc1dfe96b8",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Reinstate Data, since it was meddled with during Stacked Models\nX = train_df.drop([\"Survived\"] , axis=1)\ny = train_df[\"Survived\"]\n\n# Stratified Train/Test Split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)\nX_train.shape, y_train.shape, X_test.shape, y_test.shape\n\nevalmodel = EnsembleVoteClassifier([ensemble_models.get(key) for key in prob_models.Model[:7]], voting='soft')\nevalmodel.fit(X_train, y_train)\ny_pred = evalmodel.predict(X_test)\n# Report\nprint(\"\\n Report:\")\nprint(classification_report(y_test, y_pred))\nconfusion_matrix(y_test, y_pred)\n# Matrix\nprint(\"\\n Matrix:\")\nskplt.metrics.plot_confusion_matrix(y_pred, y_test, normalize=True)\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "4a6fe06088c4f5a0b0185be7cdcc9420dcd5d2d6",
        "_cell_guid": "f64b955c-c5e4-4949-b932-afaf1f405c39",
        "trusted": false
      },
      "cell_type": "code",
      "source": "# # Predict Submission Set and Output to CSV\n# clf = EnsembleVoteClassifier([ensemble_models.get(key) for key in prob_models.Model[:7]], voting='soft')\n# md = clf.fit(X, y)\n# df = pd.DataFrame({'PassengerId':test_df.index, 'Survived':md.predict(test_df)})\n# df.to_csv(\"Soft_Voting_7_TopModel\", index=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "3c73150834d701aa4b91c2ba414dd17112e2277b",
        "_cell_guid": "1810860e-eda6-46aa-ba98-90384387dbdc",
        "trusted": false
      },
      "cell_type": "code",
      "source": "end = time.time()\nprint(\"Notebook took %0.2f minutes to Run\"%((end - start)/60))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "e3306ffdaaac5f9ef9545f882f2b1faefc66a7e8",
        "_cell_guid": "c42793cf-5ae9-4e47-962b-6fe2551e3662"
      },
      "cell_type": "markdown",
      "source": "## Reflection\nA recurrent theme that I have observed is that the accuracy on the testing set is always higher than that of the submission set. This suggests that there is a disconnected representation of the underlying data distributions. A likely contributor to this problem is the quality of pre-processing, whose features appeared to have redundant effects, such as Title and Sex. A quick fix could be dimensionality reduction, but ultimately if proper exploration of features is conducted, then the “garbage in” problem can be minimized.\n\n### On Confusion Matrix\nAccross the board, most errors are false negatives. The model expected many of the deceased to have survived. This is perhaps why the ensembling only provided minor improvements, since it combined models with the same underlying prediction problem. \n\nNote: Deep Learning references are to be taken **LIGHTLY!**, currently reading Ian Goodfellow’s book, so this stuff is on my mind.\n\n### Thank You for making it this far! Next, I am thinking of either pursuing a wide application Regression models on another dataset, or doubling down on the exploratory analysis and feature engineering for Titanic! \n\n### *What say you?* P.S, Looking for constructive feedback :)"
    },
    {
      "metadata": {
        "_uuid": "49717a13380ff25017068937d60ca752eb7c826b",
        "_cell_guid": "c264ff4c-99e4-4fb1-88cf-e26a9ef83a58"
      },
      "cell_type": "markdown",
      "source": "## Updates\n- **UPDATE 1**: Fixing the best mode for underperformance, uploaded wrong code! [81.339 score not resovled, sorry]\n- **UPDATE 2**: Cannot reproduce optimal score, will have to gear notebook more towards reproducibility in the future.\n- ** UPDATE 3**: Added Confusion Matrix, ROC curves and Reproducibility through random_state."
    },
    {
      "metadata": {
        "_uuid": "39a2ec79010f21b4c4bd137fb7e6c8f6133f4250",
        "_cell_guid": "c0138e02-bf1a-4a3d-8270-6a4841476c04"
      },
      "cell_type": "markdown",
      "source": "## UpNext\n- To counter high false negatives, perhaps target *Specificity* (True Negative Rate) with certain models, and ensemble? Wonder what the tradeoff is. Could also experiment with the missclassification by ID (Observation), and ensemble models that differ the \n- I also want to put more work and research into stacking, since I am utilizing it sub-optimally. "
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "49f4912fbf5c286f67857949e9aa5280d11afce8",
        "_cell_guid": "5ec6bdb3-c6b4-4863-abe4-67a0308286fc",
        "trusted": false
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "name": "python",
      "mimetype": "text/x-python",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      },
      "version": "3.6.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}